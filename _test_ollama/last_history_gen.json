{"# Request\n    Generate tests, and necessary imports, only for the function '_check_storage_url', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"cli\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the function '_check_storage_url'):": "import unittest\nfrom unittest import mock\nimport os\nfrom optuna.exceptions import CLIUsageError, ExperimentalWarning\nfrom . import _check_storage_url\n\n\nclass ModuleFunctionsTests(unittest.TestCase):\n    def test_check_storage_url_provided(self) -> None:\n        self.assertEqual(_check_storage_url(\"test_url\"), \"test_url\")\n\n    def test_check_storage_url_env_set(self) -> None:\n        with mock.patch.dict(os.environ, {\"OPTUNA_STORAGE\": \"env_url\"}):\n            with self.assertWarns(ExperimentalWarning):\n                self.assertEqual(_check_storage_url(None), \"env_url\")\n\n    def test_check_storage_url_not_specified(self) -> None:\n        with self.assertRaises(CLIUsageError) as cm:\n            _check_storage_url(None)\n        self.assertEqual(str(cm.exception), \"Storage URL is not specified.\")\n", "# Request\n    Generate tests, and necessary imports, only for the function '_get_storage', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"cli\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the function '_get_storage'):": "import os\nimport unittest\nfrom unittest.mock import patch\n\nfrom optuna.storages import BaseStorage, JournalStorage, RDBStorage\nfrom optuna.storages.journal import JournalRedisBackend, JournalFileBackend\nfrom optuna.exceptions import CLIUsageError\n\nfrom cli import _get_storage\n\n\nclass TestGetStorage(unittest.TestCase):\n    def test_get_storage_with_journal_redis_backend_class(self):\n        storage = _get_storage(\"redis://example.com\", JournalRedisBackend.__name__)\n        self.assertIsInstance(storage, JournalStorage)\n        self.assertIsInstance(storage._backend, JournalRedisBackend)\n\n    def test_get_storage_with_journal_redis_storage_class(self):\n        storage = _get_storage(\"redis://example.com\", \"JournalRedisStorage\")\n        self.assertIsInstance(storage, JournalStorage)\n        self.assertIsInstance(storage._backend, JournalRedisBackend)\n\n    def test_get_storage_with_journal_file_backend_class(self):\n        with patch(\"os.path.isfile\", return_value=True):\n            storage = _get_storage(\"/path/to/file\", JournalFileBackend.__name__)\n            self.assertIsInstance(storage, JournalStorage)\n            self.assertIsInstance(storage._backend, JournalFileBackend)\n\n    def test_get_storage_with_journal_file_storage_class(self):\n        with patch(\"os.path.isfile\", return_value=True):\n            storage = _get_storage(\"/path/to/file\", \"JournalFileStorage\")\n            self.assertIsInstance(storage, JournalStorage)\n            self.assertIsInstance(storage._backend, JournalFileBackend)\n\n    def test_get_storage_with_rdb_storage_class(self):\n        storage = _get_storage(\"sqlite:///example.db\", RDBStorage.__name__)\n        self.assertIsInstance(storage, RDBStorage)\n\n    def test_get_storage_invalid_class(self):\n        with self.assertRaises(CLIUsageError):\n            _get_storage(\"redis://example.com\", \"InvalidStorageClass\")\n\n    def test_get_storage_redis_url(self):\n        storage = _get_storage(\"redis://example.com\", None)\n        self.assertIsInstance(storage, JournalStorage)\n        self.assertIsInstance(storage._backend, JournalRedisBackend)\n\n    def test_get_storage_file_url(self):\n        test_file = \"test_file.txt\"\n        with open(test_file, \"w\") as f:\n            f.write(\"\")\n        try:\n            storage = _get_storage(test_file, None)\n            self.assertIsInstance(storage._backend, JournalFileBackend)\n        finally:\n            os.remove(test_file)\n\n    def test_get_storage_rdb_url(self):\n        storage = _get_storage(\"sqlite:///example.db\", None)\n        self.assertIsInstance(storage, RDBStorage)\n\n    def test_get_storage_invalid_url(self):\n        with self.assertRaises(CLIUsageError):\n            _get_storage(\"invalid_url\", None)\n", "# Request\n    Generate tests, and necessary imports, only for the function '_format_value', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"cli\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the function '_format_value'):": "from unittest import TestCase\nimport datetime\nfrom . import _format_value\n\n\nclass ModuleFunctionsTests(TestCase):\n    def test_simple_types(self):\n        self.assertIsNone(_format_value(None))\n        self.assertEqual(_format_value(5), 5)\n        self.assertEqual(_format_value(3.14), 3.14)\n\n    def test_datetime(self):\n        dt = datetime.datetime(2023, 10, 5, 12, 34, 56)\n        self.assertEqual(_format_value(dt), \"2023-10-05 12:34:56\")\n\n    def test_list(self):\n        value = [None, 5, 3.14, datetime.datetime(2023, 1, 1, 0, 0, 0)]\n        expected = [None, 5, 3.14, \"2023-01-01 00:00:00\"]\n        self.assertEqual(_format_value(value), expected)\n\n    def test_tuple(self):\n        value = (None, 5, 3.14, datetime.datetime(2023, 1, 1, 0, 0, 0))\n        expected = (None, 5, 3.14, \"2023-01-01 00:00:00\")\n        self.assertEqual(_format_value(value), expected)\n\n    def test_dict(self):\n        value = {1: None, \"key\": datetime.datetime(2023, 1, 1, 0, 0, 0)}\n        expected = {1: None, \"key\": \"2023-01-01 00:00:00\"}\n        self.assertEqual(_format_value(value), expected)\n\n    def test_other_types(self):\n        self.assertEqual(_format_value(\"string\"), \"string\")\n        self.assertEqual(_format_value(object()), str(object()))\n", "# Request\n    Generate tests, and necessary imports, only for the function '_convert_to_dict', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"cli\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the function '_convert_to_dict'):": "import unittest\nimport datetime\nfrom optuna.cli import _convert_to_dict\n\nclass TestConvertToDict(unittest.TestCase):\n\n    def test_flatten_with_subkeys(self):\n        columns = [('study', 'name'), ('study', 'direction')]\n        records = [\n            {('study', 'name'): 'study1', ('study', 'direction'): 'maximize'}\n        ]\n        expected_header = ['study_name', 'study_direction']\n        expected_data = [{'study_name': 'study1', 'study_direction': 'maximize'}]\n        result, header = _convert_to_dict(records, columns, True)\n        self.assertEqual(header, expected_header)\n        self.assertEqual(result, expected_data)\n\n    def test_flatten_with_lists(self):\n        columns = [('params', '')]\n        records = [\n            {('params', ''): [1, 2, 3]}\n        ]\n        expected_header = ['params_0', 'params_1', 'params_2']\n        expected_data = [{'params_0': 1, 'params_1': 2, 'params_2': 3}]\n        result, header = _convert_to_dict(records, columns, True)\n        self.assertEqual(header, expected_header)\n        self.assertEqual(result, expected_data)\n\n    def test_flatten_with_variable_length_lists(self):\n        columns = [('params', '')]\n        records = [\n            {('params', ''): [1, 2]},\n            {('params', ''): [3]}\n        ]\n        expected_header = ['params_0', 'params_1']\n        expected_data = [\n            {'params_0': 1, 'params_1': 2},\n            {'params_0': 3, 'params_1': None}\n        ]\n        result, header = _convert_to_dict(records, columns, True)\n        self.assertEqual(header, expected_header)\n        self.assertEqual(result, expected_data)\n\n    def test_non_flatten(self):\n        columns = [('study', 'name'), ('study', 'direction')]\n        records = [\n            {('study', 'name'): 'study1', ('study', 'direction'): 'maximize'}\n        ]\n        expected_header = ['study']\n        expected_data = [{'study': {'name': 'study1', 'direction': 'maximize'}}]\n        result, header = _convert_to_dict(records, columns, False)\n        self.assertEqual(header, expected_header)\n        self.assertEqual(result, expected_data)\n\n    def test_non_flatten_with_list_indices(self):\n        columns = [('params', 0), ('params', 1)]\n        records = [\n            {('params', 0): 1, ('params', 1): 2}\n        ]\n        expected_header = ['params']\n        expected_data = [{'params': [1, 2]}]\n        result, header = _convert_to_dict(records, columns, False)\n        self.assertEqual(header, expected_header)\n        self.assertEqual(result, expected_data)\n\n    def test_missing_columns(self):\n        columns = [('study', 'name'), ('study', 'direction')]\n        records = [\n            {('study', 'name'): 'study1'}  # Missing 'direction'\n        ]\n        expected_header = ['study_name', 'study_direction']\n        expected_data = [{'study_name': 'study1', 'study_direction': None}]\n        result, header = _convert_to_dict(records, columns, True)\n        self.assertEqual(header, expected_header)\n        self.assertEqual(result, expected_data)\n\n    def test_datetime_formatting(self):\n        now = datetime.datetime(2023, 10, 5, 12, 30, 45)\n        columns = [('datetime', '')]\n        records = [\n            {('datetime', ''): now}\n        ]\n        expected_header = ['datetime_0']\n        expected_data = [{'datetime_0': '2023-10-05 12:30:45'}]\n        result, header = _convert_to_dict(records, columns, True)\n        self.assertEqual(header, expected_header)\n        self.assertEqual(result, expected_data)\n", "# Request\n    Generate tests, and necessary imports, only for the function 'main', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"cli\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the function 'main'):": "import unittest\nfrom unittest import mock\nimport sys\nfrom optuna.cli import main\nfrom optuna.exceptions import CLIUsageError\n\n\nclass MainFunctionTests(unittest.TestCase):\n    def test_valid_command(self):\n        with mock.patch.object(sys, \"argv\", [\"optuna\", \"study\", \"create\"]):\n            with mock.patch(\"optuna.cli.argparse.ArgumentParser.parse_args\") as mock_parse:\n                mock_args = mock.Mock()\n                mock_args.handler = lambda x: 0\n                mock_parse.return_value = mock_args\n                exit_code = main()\n                self.assertEqual(exit_code, 0)\n\n    def test_invalid_command(self):\n        with mock.patch.object(sys, \"argv\", [\"optuna\", \"-v\"]):\n            exit_code = main()\n            self.assertEqual(exit_code, 1)\n\n    def test_cli_usage_error(self):\n        with mock.patch.object(sys, \"argv\", [\"optuna\", \"study\", \"invalid\"]):\n            with mock.patch(\"optuna.cli.argparse.ArgumentParser.parse_args\") as mock_parse:\n                mock_args = mock.Mock()\n                mock_args.handler = mock.Mock(\n                    side_effect=CLIUsageError(\"Test error\")\n                )\n                mock_parse.return_value = mock_args\n                exit_code = main()\n                self.assertEqual(exit_code, 1)\n\n    def test_cli_usage_error_with_debug(self):\n        with mock.patch.object(sys, \"argv\", [\"optuna\", \"--debug\", \"study\", \"invalid\"]):\n            with mock.patch(\"optuna.cli.argparse.ArgumentParser.parse_args\") as mock_parse:\n                mock_args = mock.Mock()\n                mock_args.handler = mock.Mock(\n                    side_effect=CLIUsageError(\"Test error\")\n                )\n                mock_args.debug = True\n                mock_parse.return_value = mock_args\n                exit_code = main()\n                self.assertEqual(exit_code, 1)\n\n    def test_missing_subcommand(self):\n        with mock.patch.object(sys, \"argv\", [\"optuna\", \"-q\"]):\n            exit_code = main()\n            self.assertEqual(exit_code, 1)\n", "# Request\n    Generate tests, and necessary imports, only for the method '__init__', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"CellValue\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the method '__init__' of the class \"CellValue\"):": "import unittest\nfrom datetime import datetime\n\nfrom cli import CellValue, ValueType\n\n\nclass CellValueTest(unittest.TestCase):\n    def test_init_with_none(self) -> None:\n        cell = CellValue(None)\n        self.assertIsNone(cell.value)\n        self.assertEqual(cell.value_type, ValueType.NONE)\n\n    def test_init_with_int(self) -> None:\n        cell = CellValue(5)\n        self.assertEqual(cell.value, 5)\n        self.assertEqual(cell.value_type, ValueType.NUMERIC)\n\n    def test_init_with_float(self) -> None:\n        cell = CellValue(3.14)\n        self.assertEqual(cell.value, 3.14)\n        self.assertEqual(cell.value_type, ValueType.NUMERIC)\n\n    def test_init_with_string(self) -> None:\n        cell = CellValue(\"hello\")\n        self.assertEqual(cell.value, \"hello\")\n        self.assertEqual(cell.value_type, ValueType.STRING)\n\n    def test_init_with_boolean(self) -> None:\n        cell = CellValue(True)\n        self.assertIs(cell.value, True)\n        self.assertEqual(cell.value_type, ValueType.NUMERIC)\n\n    def test_init_with_datetime(self) -> None:\n        now = datetime.now()\n        cell = CellValue(now)\n        self.assertEqual(cell.value, now)\n        self.assertEqual(cell.value_type, ValueType.STRING)\n", "# Request\n    Generate tests, and necessary imports, only for the method '__str__', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"CellValue\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the method '__str__' of the class \"CellValue\"):": "import unittest\nfrom datetime import datetime\nfrom cli import CellValue\n\nclass TestCellValueStr(unittest.TestCase):\n    def test_none_value(self):\n        cell = CellValue(None)\n        self.assertEqual(str(cell), \"None\")\n\n    def test_numeric_value_int(self):\n        cell = CellValue(42)\n        self.assertEqual(str(cell), \"42\")\n\n    def test_numeric_value_float(self):\n        cell = CellValue(3.14)\n        self.assertEqual(str(cell), \"3.14\")\n\n    def test_string_value(self):\n        cell = CellValue(\"example\")\n        self.assertEqual(str(cell), \"example\")\n\n    def test_boolean_value(self):\n        cell = CellValue(True)\n        self.assertEqual(str(cell), \"True\")\n\n    def test_datetime_value(self):\n        dt = datetime(2023, 10, 5, 14, 30, 22)\n        cell = CellValue(dt)\n        self.assertEqual(str(cell), \"2023-10-05 14:30:22\")\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "# Request\n    Generate tests, and necessary imports, only for the method 'width', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"CellValue\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the method 'width' of the class \"CellValue\"):": "import unittest\nfrom datetime import datetime\nfrom cli import CellValue\n\n\nclass TestCellValueWidth(unittest.TestCase):\n    def test_none_value(self):\n        cell = CellValue(None)\n        self.assertEqual(cell.width(), 4)\n\n    def test_integer_value(self):\n        cell = CellValue(123)\n        self.assertEqual(cell.width(), 3)\n\n    def test_float_value(self):\n        cell = CellValue(3.14)\n        self.assertEqual(cell.width(), 4)\n\n    def test_string_value(self):\n        cell = CellValue(\"hello\")\n        self.assertEqual(cell.width(), 5)\n\n    def test_datetime_value(self):\n        dt = datetime(2023, 10, 5, 12, 34, 56)\n        cell = CellValue(dt)\n        self.assertEqual(cell.width(), 19)\n", "# Request\n    Generate tests, and necessary imports, only for the method 'get_string', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"CellValue\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\cli\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\n\"\"\"Optuna CLI module.\nIf you want to add a new command, you also need to update the constant `_COMMANDS`\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nfrom argparse import ArgumentParser\nfrom argparse import Namespace\nimport datetime\nfrom enum import Enum\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any\nimport warnings\n\nimport sqlalchemy.exc\nimport yaml\n\nimport optuna\nfrom optuna._imports import _LazyImport\nfrom optuna.exceptions import CLIUsageError\nfrom optuna.exceptions import ExperimentalWarning\nfrom optuna.storages import BaseStorage\nfrom optuna.storages import JournalFileStorage\nfrom optuna.storages import JournalRedisStorage\nfrom optuna.storages import JournalStorage\nfrom optuna.storages import RDBStorage\nfrom optuna.storages.journal import JournalFileBackend\nfrom optuna.storages.journal import JournalRedisBackend\nfrom optuna.trial import TrialState\n\n\n_dataframe = _LazyImport(\"optuna.study._dataframe\")\n\n_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\ndef _check_storage_url(storage_url: str | None) -> str:\n    if storage_url is not None:\n        return storage_url\n\n    env_storage = os.environ.get(\"OPTUNA_STORAGE\")\n    if env_storage is not None:\n        warnings.warn(\n            \"Specifying the storage url via 'OPTUNA_STORAGE' environment variable\"\n            \" is an experimental feature. The interface can change in the future.\",\n            ExperimentalWarning,\n        )\n        return env_storage\n    raise CLIUsageError(\"Storage URL is not specified.\")\n\n\ndef _get_storage(storage_url: str | None, storage_class: str | None) -> BaseStorage:\n    storage_url = _check_storage_url(storage_url)\n    if storage_class:\n        if storage_class == JournalRedisBackend.__name__:\n            return JournalStorage(JournalRedisBackend(storage_url))\n        if storage_class == JournalRedisStorage.__name__:\n            return JournalStorage(JournalRedisStorage(storage_url))\n        if storage_class == JournalFileBackend.__name__:\n            return JournalStorage(JournalFileBackend(storage_url))\n        if storage_class == JournalFileStorage.__name__:\n            return JournalStorage(JournalFileStorage(storage_url))\n        if storage_class == RDBStorage.__name__:\n            return RDBStorage(storage_url)\n        raise CLIUsageError(\"Unsupported storage class\")\n\n    if storage_url.startswith(\"redis\"):\n        return JournalStorage(JournalRedisBackend(storage_url))\n    if os.path.isfile(storage_url):\n        return JournalStorage(JournalFileBackend(storage_url))\n    try:\n        return RDBStorage(storage_url)\n    except sqlalchemy.exc.ArgumentError:\n        raise CLIUsageError(\"Failed to guess storage class from storage_url\")\n\n\ndef _format_value(value: Any) -> Any:\n    #  Format value that can be serialized to JSON or YAML.\n    if value is None or isinstance(value, (int, float)):\n        return value\n    elif isinstance(value, datetime.datetime):\n        return value.strftime(_DATETIME_FORMAT)\n    elif isinstance(value, list):\n        return list(_format_value(v) for v in value)\n    elif isinstance(value, tuple):\n        return tuple(_format_value(v) for v in value)\n    elif isinstance(value, dict):\n        return {_format_value(k): _format_value(v) for k, v in value.items()}\n    else:\n        return str(value)\n\n\ndef _convert_to_dict(\n    records: list[dict[tuple[str, str], Any]], columns: list[tuple[str, str]], flatten: bool\n) -> tuple[list[dict[str, Any]], list[str]]:\n    header = []\n    ret = []\n    if flatten:\n        for column in columns:\n            if column[1] != \"\":\n                header.append(f\"{column[0]}_{column[1]}\")\n            elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                max_length = 0\n                for record in records:\n                    if column in record:\n                        max_length = max(max_length, len(record[column]))\n                for i in range(max_length):\n                    header.append(f\"{column[0]}_{i}\")\n            else:\n                header.append(column[0])\n        for record in records:\n            row = {}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if column[1] != \"\":\n                    row[f\"{column[0]}_{column[1]}\"] = value\n                elif any(isinstance(record.get(column), (list, tuple)) for record in records):\n                    for i, v in enumerate(value):\n                        row[f\"{column[0]}_{i}\"] = v\n                else:\n                    row[f\"{column[0]}\"] = value\n            ret.append(row)\n    else:\n        for column in columns:\n            if column[0] not in header:\n                header.append(column[0])\n        for record in records:\n            attrs: dict[str, Any] = {column_name: {} for column_name in header}\n            for column in columns:\n                if column not in record:\n                    continue\n                value = _format_value(record[column])\n                if isinstance(column[1], int):\n                    # Reconstruct list of values. `_dataframe._create_records_and_aggregate_column`\n                    # returns indices of list as the second key of column.\n                    if attrs[column[0]] == {}:\n                        attrs[column[0]] = []\n                    attrs[column[0]] += [None] * max(column[1] + 1 - len(attrs[column[0]]), 0)\n                    attrs[column[0]][column[1]] = value\n                elif column[1] != \"\":\n                    attrs[column[0]][column[1]] = value\n                else:\n                    attrs[column[0]] = value\n            ret.append(attrs)\n\n    return ret, header\n\n\nclass ValueType(Enum):\n    NONE = 0\n    NUMERIC = 1\n    STRING = 2\n\n\nclass CellValue:\n    def __init__(self, value: Any) -> None:\n        self.value = value\n        if value is None:\n            self.value_type = ValueType.NONE\n        elif isinstance(value, (int, float)):\n            self.value_type = ValueType.NUMERIC\n        else:\n            self.value_type = ValueType.STRING\n\n    def __str__(self) -> str:\n        if isinstance(self.value, datetime.datetime):\n            return self.value.strftime(_DATETIME_FORMAT)\n        else:\n            return str(self.value)\n\n    def width(self) -> int:\n        return len(str(self.value))\n\n    def get_string(self, value_type: ValueType, width: int) -> str:\n        value = str(self.value)\n        if self.value is None:\n            return \" \" * width\n        elif value_type == ValueType.NUMERIC:\n            return f\"{value:>{width}}\"\n        else:\n            return f\"{value:<{width}}\"\n\n\ndef main() -> int:\n    main_parser, command_name_to_subparser = _get_parser()\n\n    argv = sys.argv\n    preprocessed_argv = _preprocess_argv(argv)\n    args = main_parser.parse_args(preprocessed_argv)\n\n    _set_verbosity(args)\n    _set_log_file(args)\n\n    logger = logging.getLogger(\"optuna\")\n    try:\n        return args.handler(args)\n    except CLIUsageError as e:\n        if args.debug:\n            logger.exception(e)\n        else:\n            logger.error(e)\n            # This code is required to show help for each subcommand.\n            # NOTE: the first element of `preprocessed_argv` is command name.\n            command_name_to_subparser[preprocessed_argv[0]].print_help()\n        return 1\n    except AttributeError:\n        # Exception for the case -v/--verbose/-q/--quiet/--log-file/--debug\n        # without any subcommand.\n        argv_str = \" \".join(argv[1:])\n        logger.error(f\"'{argv_str}' is not an optuna command. see 'optuna --help'\")\n        main_parser.print_help()\n        return 1\n\n```\n\n# Answer (only for the method 'get_string' of the class \"CellValue\"):": "import unittest\nfrom cli import CellValue, ValueType\n\nclass TestCellValueGetString(unittest.TestCase):\n    def test_none_value_returns_spaces(self):\n        cell = CellValue(None)\n        self.assertEqual(cell.get_string(ValueType.NUMERIC, 5), \"     \")\n        self.assertEqual(cell.get_string(ValueType.STRING, 3), \"   \")\n\n    def test_numeric_value_with_numeric_type_right_aligned(self):\n        cell = CellValue(42)\n        self.assertEqual(cell.get_string(ValueType.NUMERIC, 5), \"   42\")\n        cell = CellValue(3.14)\n        self.assertEqual(cell.get_string(ValueType.NUMERIC, 7), \"    3.14\")\n\n    def test_string_value_with_string_type_left_aligned(self):\n        cell = CellValue(\"hello\")\n        self.assertEqual(cell.get_string(ValueType.STRING, 7), \"hello  \")\n        cell = CellValue(\"abc\")\n        self.assertEqual(cell.get_string(ValueType.STRING, 5), \"abc  \")\n\n    def test_numeric_value_with_string_type_left_aligned(self):\n        cell = CellValue(123)\n        self.assertEqual(cell.get_string(ValueType.STRING, 6), \"123   \")\n        cell = CellValue(45.6)\n        self.assertEqual(cell.get_string(ValueType.STRING, 8), \"45.6    \")\n\n    def test_string_value_with_numeric_type_right_aligned(self):\n        cell = CellValue(\"test\")\n        self.assertEqual(cell.get_string(ValueType.NUMERIC, 6), \"  test\")\n        cell = CellValue(\"x\")\n        self.assertEqual(cell.get_string(ValueType.NUMERIC, 3), \"  x\")\n\n    def test_value_longer_than_width(self):\n        cell = CellValue(\"longer\")\n        self.assertEqual(cell.get_string(ValueType.STRING, 4), \"longer\")\n        self.assertEqual(cell.get_string(ValueType.NUMERIC, 4), \"longer\")\n\n    def test_datetime_value(self):\n        from datetime import datetime\n        dt = datetime(2023, 1, 1, 12, 30, 45)\n        cell = CellValue(dt)\n        self.assertEqual(cell.get_string(ValueType.STRING, 19), \"2023-01-01 12:30:45   \")\n        self.assertEqual(cell.get_string(ValueType.NUMERIC, 19), \"2023-01-01 12:30:45\")\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "# Request\n    Generate tests, and necessary imports, only for the function '_categorical_choice_equal', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function '_categorical_choice_equal'):": "import math\nfrom unittest import TestCase\n\nfrom optuna.distributions import _categorical_choice_equal\n\n\nclass ModuleFunctionsTests(TestCase):\n    def test_categorical_choice_equal(self):\n        # Test equal values of the same type.\n        self.assertTrue(_categorical_choice_equal(5, 5))\n        self.assertTrue(_categorical_choice_equal(3.14, 3.14))\n        self.assertTrue(_categorical_choice_equal(\"a\", \"a\"))\n        self.assertTrue(_categorical_choice_equal(None, None))\n\n        # Test equal values of different types.\n        self.assertTrue(_categorical_choice_equal(5, 5.0))\n        self.assertTrue(_categorical_choice_equal(True, 1.0))\n        self.assertTrue(_categorical_choice_equal(False, 0.0))\n        self.assertTrue(_categorical_choice_equal(True, 1))\n        self.assertTrue(_categorical_choice_equal(False, 0))\n\n        # Test NaN values.\n        nan = float(\"nan\")\n        self.assertTrue(_categorical_choice_equal(nan, nan))\n\n        # Test unequal values.\n        self.assertFalse(_categorical_choice_equal(5, 6))\n        self.assertFalse(_categorical_choice_equal(5, 5.1))\n        self.assertFalse(_categorical_choice_equal(\"a\", \"b\"))\n        self.assertFalse(_categorical_choice_equal(None, 5))\n        self.assertFalse(_categorical_choice_equal(5, None))\n\n        # Test one NaN and one non-NaN.\n        self.assertFalse(_categorical_choice_equal(nan, 5))\n        self.assertFalse(_categorical_choice_equal(5, nan))\n\n        # Test different types with unequal numeric values.\n        self.assertFalse(_categorical_choice_equal(True, 2.0))\n        self.assertFalse(_categorical_choice_equal(1.0, True))\n", "# Request\n    Generate tests, and necessary imports, only for the function 'json_to_distribution', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function 'json_to_distribution'):": "import json\nfrom unittest import TestCase\n\nfrom optuna.distributions import CategoricalDistribution\nfrom optuna.distributions import FloatDistribution\nfrom optuna.distributions import IntDistribution\nfrom optuna.distributions import json_to_distribution\n\n\nclass JsonToDistributionTestCase(TestCase):\n    def test_new_format_categorical(self):\n        json_str = json.dumps(\n            {\n                \"name\": \"CategoricalDistribution\",\n                \"attributes\": {\"choices\": [1, 2, 3]},\n            }\n        )\n        dist = json_to_distribution(json_str)\n        self.assertIsInstance(dist, CategoricalDistribution)\n        self.assertEqual(dist.choices, (1, 2, 3))\n\n    def test_new_format_float(self):\n        json_str = json.dumps(\n            {\n                \"name\": \"FloatDistribution\",\n                \"attributes\": {\"low\": 1.0, \"high\": 10.0, \"log\": False, \"step\": None},\n            }\n        )\n        dist = json_to_distribution(json_str)\n        self.assertIsInstance(dist, FloatDistribution)\n        self.assertEqual(dist.low, 1.0)\n        self.assertEqual(dist.high, 10.0)\n        self.assertFalse(dist.log)\n        self.assertIsNone(dist.step)\n\n    def test_new_format_int(self):\n        json_str = json.dumps(\n            {\n                \"name\": \"IntDistribution\",\n                \"attributes\": {\"low\": 1, \"high\": 10, \"log\": False, \"step\": 1},\n            }\n        )\n        dist = json_to_distribution(json_str)\n        self.assertIsInstance(dist, IntDistribution)\n        self.assertEqual(dist.low, 1)\n        self.assertEqual(dist.high, 10)\n        self.assertFalse(dist.log)\n        self.assertEqual(dist.step, 1)\n\n    def test_old_format_categorical(self):\n        json_str = json.dumps({\"type\": \"categorical\", \"choices\": [1, 2, 3]})\n        dist = json_to_distribution(json_str)\n        self.assertIsInstance(dist, CategoricalDistribution)\n        self.assertEqual(dist.choices, (1, 2, 3))\n\n    def test_old_format_float(self):\n        json_str = json.dumps(\n            {\"type\": \"float\", \"low\": 1.0, \"high\": 10.0, \"log\": False, \"step\": None}\n        )\n        dist = json_to_distribution(json_str)\n        self.assertIsInstance(dist, FloatDistribution)\n        self.assertEqual(dist.low, 1.0)\n        self.assertEqual(dist.high, 10.0)\n        self.assertFalse(dist.log)\n        self.assertIsNone(dist.step)\n\n    def test_old_format_int(self):\n        json_str = json.dumps({\"type\": \"int\", \"low\": 1, \"high\": 10, \"log\": False, \"step\": 1})\n        dist = json_to_distribution(json_str)\n        self.assertIsInstance(dist, IntDistribution)\n        self.assertEqual(dist.low, 1)\n        self.assertEqual(dist.high, 10)\n        self.assertFalse(dist.log)\n        self.assertEqual(dist.step, 1)\n\n    def test_unknown_class_new_format(self):\n        json_str = json.dumps({\"name\": \"UnknownDistribution\", \"attributes\": {}})\n        with self.assertRaises(ValueError) as cm:\n            json_to_distribution(json_str)\n        self.assertEqual(str(cm.exception), \"Unknown distribution class: {'name': 'UnknownDistribution', 'attributes': {}}\")\n\n    def test_unknown_type_old_format(self):\n        json_str = json.dumps({\"type\": \"unknown\", \"low\": 1, \"high\": 10})\n        with self.assertRaises(ValueError) as cm:\n            json_to_distribution(json_str)\n        self.assertEqual(str(cm.exception), \"Unknown distribution type: unknown\")\n", "# Request\n    Generate tests, and necessary imports, only for the function 'distribution_to_json', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function 'distribution_to_json'):": "import json\nfrom unittest import TestCase\n\nfrom optuna.distributions import (\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    IntDistribution,\n    IntUniformDistribution,\n    IntLogUniformDistribution,\n    CategoricalDistribution,\n    distribution_to_json,\n)\n\n\nclass DistributionToJsonTest(TestCase):\n    def test_float_distribution(self):\n        dist = FloatDistribution(low=1.0, high=2.0, log=False, step=None)\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"FloatDistribution\")\n        self.assertEqual(\n            data[\"attributes\"],\n            {\"low\": 1.0, \"high\": 2.0, \"log\": False, \"step\": None},\n        )\n\n    def test_uniform_distribution(self):\n        dist = UniformDistribution(low=1.0, high=2.0)\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"UniformDistribution\")\n        self.assertEqual(data[\"attributes\"], {\"low\": 1.0, \"high\": 2.0})\n\n    def test_log_uniform_distribution(self):\n        dist = LogUniformDistribution(low=0.1, high=1.0)\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"LogUniformDistribution\")\n        self.assertEqual(data[\"attributes\"], {\"low\": 0.1, \"high\": 1.0})\n\n    def test_discrete_uniform_distribution(self):\n        dist = DiscreteUniformDistribution(low=0.0, high=1.0, q=0.1)\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"DiscreteUniformDistribution\")\n        self.assertEqual(\n            data[\"attributes\"], {\"low\": 0.0, \"high\": 1.0, \"q\": 0.1}\n        )\n\n    def test_int_distribution(self):\n        dist = IntDistribution(low=1, high=10, log=False, step=2)\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"IntDistribution\")\n        self.assertEqual(\n            data[\"attributes\"], {\"low\": 1, \"high\": 10, \"log\": False, \"step\": 2}\n        )\n\n    def test_int_uniform_distribution(self):\n        dist = IntUniformDistribution(low=1, high=10, step=2)\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"IntUniformDistribution\")\n        self.assertEqual(data[\"attributes\"], {\"low\": 1, \"high\": 10, \"step\": 2})\n\n    def test_int_log_uniform_distribution(self):\n        dist = IntLogUniformDistribution(low=2, high=8, step=1)\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"IntLogUniformDistribution\")\n        self.assertEqual(data[\"attributes\"], {\"low\": 2, \"high\": 8, \"step\": 1})\n\n    def test_categorical_distribution(self):\n        choices = [None, True, 42, 3.14, \"hello\"]\n        dist = CategoricalDistribution(choices=choices)\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"CategoricalDistribution\")\n        self.assertEqual(data[\"attributes\"], {\"choices\": tuple(choices)})\n\n    def test_categorical_distribution_single_choice(self):\n        dist = CategoricalDistribution(choices=[\"single\"])\n        json_str = distribution_to_json(dist)\n        data = json.loads(json_str)\n        self.assertEqual(data[\"name\"], \"CategoricalDistribution\")\n        self.assertEqual(data[\"attributes\"], {\"choices\": (\"single\",)})\n", "# Request\n    Generate tests, and necessary imports, only for the function 'check_distribution_compatibility', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function 'check_distribution_compatibility'):": "import unittest\n\nfrom distributions import check_distribution_compatibility, FloatDistribution, IntDistribution, CategoricalDistribution\n\n\nclass CheckDistributionCompatibilityTest(unittest.TestCase):\n    def test_same_distribution(self):\n        dist1 = FloatDistribution(0, 1)\n        dist2 = FloatDistribution(0, 1)\n        check_distribution_compatibility(dist1, dist2)\n\n    def test_different_distribution_classes(self):\n        dist1 = FloatDistribution(0, 1)\n        dist2 = IntDistribution(0, 1)\n        with self.assertRaises(ValueError):\n            check_distribution_compatibility(dist1, dist2)\n\n    def test_float_different_log(self):\n        dist1 = FloatDistribution(0, 1, log=True)\n        dist2 = FloatDistribution(0, 1, log=False)\n        with self.assertRaises(ValueError):\n            check_distribution_compatibility(dist1, dist2)\n\n    def test_int_different_log(self):\n        dist1 = IntDistribution(1, 10, log=True, step=1)\n        dist2 = IntDistribution(1, 10, log=False, step=1)\n        with self.assertRaises(ValueError):\n            check_distribution_compatibility(dist1, dist2)\n\n    def test_categorical_same_choices(self):\n        dist1 = CategoricalDistribution([1, 2, 3])\n        dist2 = CategoricalDistribution([1, 2, 3])\n        check_distribution_compatibility(dist1, dist2)\n\n    def test_categorical_different_choices(self):\n        dist1 = CategoricalDistribution([1, 2, 3])\n        dist2 = CategoricalDistribution([1, 2, 4])\n        with self.assertRaises(ValueError):\n            check_distribution_compatibility(dist1, dist2)\n\n    def test_categorical_different_order(self):\n        dist1 = CategoricalDistribution([1, 2, 3])\n        dist2 = CategoricalDistribution([3, 2, 1])\n        with self.assertRaises(ValueError):\n            check_distribution_compatibility(dist1, dist2)\n", "# Request\n    Generate tests, and necessary imports, only for the function '_adjust_discrete_uniform_high', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function '_adjust_discrete_uniform_high'):": "import pytest\nimport decimal\nfrom optuna.distributions import _adjust_discrete_uniform_high\n\n\nclass TestAdjustDiscreteUniformHigh:\n    def test_divisible_range(self):\n        assert _adjust_discrete_uniform_high(0.0, 4.0, 2.0) == 4.0\n\n    def test_non_divisible_range(self):\n        with pytest.warns(UserWarning):\n            assert _adjust_discrete_uniform_high(0.0, 5.0, 2.0) == 4.0\n\n    def test_decimal_divisible(self):\n        assert _adjust_discrete_uniform_high(1.0, 4.5, 0.5) == 4.5\n\n    def test_decimal_non_divisible(self):\n        with pytest.warns(UserWarning):\n            assert _adjust_discrete_uniform_high(1.0, 4.6, 0.5) == pytest.approx(4.5)\n\n    def test_another_decimal_case(self):\n        with pytest.warns(UserWarning):\n            assert _adjust_discrete_uniform_high(0.0, 0.35, 0.1) == pytest.approx(0.3)\n\n    def test_exact_step(self):\n        assert _adjust_discrete_uniform_high(0.0, 0.1, 0.1) == pytest.approx(0.1)\n", "# Request\n    Generate tests, and necessary imports, only for the function '_adjust_int_uniform_high', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function '_adjust_int_uniform_high'):": "import unittest\nimport warnings\n\nfrom distributions import _adjust_int_uniform_high\n\n\nclass TestAdjustIntUniformHigh(unittest.TestCase):\n    def test_divisible_case(self):\n        # Test when (high - low) is divisible by step.\n        self.assertEqual(_adjust_int_uniform_high(0, 10, 2), 10)\n        self.assertEqual(_adjust_int_uniform_high(5, 5, 3), 5)  # High equals low.\n\n    def test_non_divisible_case(self):\n        # Test when (high - low) is not divisible by step.\n        with self.assertWarns(UserWarning):\n            self.assertEqual(_adjust_int_uniform_high(0, 5, 2), 4)\n        with self.assertWarns(UserWarning):\n            self.assertEqual(_adjust_int_uniform_high(1, 6, 2), 5)\n        with self.assertWarns(UserWarning):\n            self.assertEqual(_adjust_int_uniform_high(0, 4, 3), 3)\n\n    def test_step_1(self):\n        # Test with step=1, which should not trigger a warning if divisible.\n        self.assertEqual(_adjust_int_uniform_high(2, 3, 1), 3)\n        # Non-divisible case with step=1.\n        with self.assertWarns(UserWarning):\n            self.assertEqual(_adjust_int_uniform_high(2, 5, 2), 4)\n\n    def test_no_warning_for_divisible(self):\n        # Ensure no warning is raised when the range is divisible by step.\n        with warnings.catch_warnings(record=True) as w:\n            _adjust_int_uniform_high(0, 10, 5)  # 10-0=10, step 5: 10%5=0\n            self.assertEqual(len(w), 0)\n", "# Request\n    Generate tests, and necessary imports, only for the function '_get_single_value', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function '_get_single_value'):": "import unittest\n\nfrom optuna.distributions import (\n    _get_single_value,\n    FloatDistribution,\n    IntDistribution,\n    CategoricalDistribution,\n    UniformDistribution,\n    IntUniformDistribution,\n)\n\n\nclass TestGetSingleValue(unittest.TestCase):\n    def test_float_distribution_single_low_equal_high(self):\n        dist = FloatDistribution(2.0, 2.0)\n        self.assertTrue(dist.single())\n        self.assertEqual(_get_single_value(dist), 2.0)\n\n    def test_float_distribution_with_step_single(self):\n        dist = FloatDistribution(low=1.0, high=1.2, step=0.3)\n        self.assertTrue(dist.single())\n        self.assertEqual(_get_single_value(dist), 1.0)\n\n    def test_int_distribution_single_low_equal_high(self):\n        dist = IntDistribution(3, 3)\n        self.assertTrue(dist.single())\n        self.assertEqual(_get_single_value(dist), 3)\n\n    def test_int_distribution_with_step_single(self):\n        dist = IntDistribution(low=2, high=3, step=2)\n        self.assertTrue(dist.single())\n        self.assertEqual(_get_single_value(dist), 2)\n\n    def test_categorical_distribution_single_choice(self):\n        dist = CategoricalDistribution([42])\n        self.assertTrue(dist.single())\n        self.assertEqual(_get_single_value(dist), 42)\n\n    def test_uniform_distribution_single(self):\n        dist = UniformDistribution(5.0, 5.0)\n        self.assertTrue(dist.single())\n        self.assertEqual(_get_single_value(dist), 5.0)\n\n    def test_int_uniform_distribution_single(self):\n        dist = IntUniformDistribution(4, 4)\n        self.assertTrue(dist.single())\n        self.assertEqual(_get_single_value(dist), 4)\n", "# Request\n    Generate tests, and necessary imports, only for the function '_convert_old_distribution_to_new_distribution', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function '_convert_old_distribution_to_new_distribution'):": "import unittest\nfrom unittest import mock\nimport warnings\n\nfrom optuna.distributions import (\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    IntUniformDistribution,\n    IntLogUniformDistribution,\n    CategoricalDistribution,\n    _convert_old_distribution_to_new_distribution,\n    FloatDistribution,\n    IntDistribution,\n)\n\n\nclass TestConvertOldDistributionToNewDistribution(unittest.TestCase):\n    def test_uniform_distribution(self):\n        old_dist = UniformDistribution(low=1.0, high=2.0)\n        new_dist = _convert_old_distribution_to_new_distribution(old_dist)\n        self.assertIsInstance(new_dist, FloatDistribution)\n        self.assertEqual(new_dist.low, 1.0)\n        self.assertEqual(new_dist.high, 2.0)\n        self.assertFalse(new_dist.log)\n        self.assertIsNone(new_dist.step)\n        self.assertNotEqual(new_dist, old_dist)\n\n        with self.assertWarns(FutureWarning):\n            _convert_old_distribution_to_new_distribution(old_dist, suppress_warning=False)\n\n    def test_log_uniform_distribution(self):\n        old_dist = LogUniformDistribution(low=1.0, high=2.0)\n        new_dist = _convert_old_distribution_to_new_distribution(old_dist)\n        self.assertIsInstance(new_dist, FloatDistribution)\n        self.assertEqual(new_dist.low, 1.0)\n        self.assertEqual(new_dist.high, 2.0)\n        self.assertTrue(new_dist.log)\n        self.assertIsNone(new_dist.step)\n        self.assertNotEqual(new_dist, old_dist)\n\n        with self.assertWarns(FutureWarning):\n            _convert_old_distribution_to_new_distribution(old_dist, suppress_warning=False)\n\n    def test_discrete_uniform_distribution(self):\n        old_dist = DiscreteUniformDistribution(low=1.0, high=3.0, q=0.5)\n        new_dist = _convert_old_distribution_to_new_distribution(old_dist)\n        self.assertIsInstance(new_dist, FloatDistribution)\n        self.assertEqual(new_dist.low, 1.0)\n        self.assertEqual(new_dist.high, 3.0)\n        self.assertFalse(new_dist.log)\n        self.assertEqual(new_dist.step, 0.5)\n        self.assertNotEqual(new_dist, old_dist)\n\n        with self.assertWarns(FutureWarning):\n            _convert_old_distribution_to_new_distribution(old_dist, suppress_warning=False)\n\n    def test_int_uniform_distribution(self):\n        old_dist = IntUniformDistribution(low=1, high=5, step=2)\n        new_dist = _convert_old_distribution_to_new_distribution(old_dist)\n        self.assertIsInstance(new_dist, IntDistribution)\n        self.assertEqual(new_dist.low, 1)\n        self.assertEqual(new_dist.high, 5)\n        self.assertFalse(new_dist.log)\n        self.assertEqual(new_dist.step, 2)\n        self.assertNotEqual(new_dist, old_dist)\n\n        with self.assertWarns(FutureWarning):\n            _convert_old_distribution_to_new_distribution(old_dist, suppress_warning=False)\n\n    def test_int_log_uniform_distribution(self):\n        old_dist = IntLogUniformDistribution(low=2, high=8, step=1)\n        new_dist = _convert_old_distribution_to_new_distribution(old_dist)\n        self.assertIsInstance(new_dist, IntDistribution)\n        self.assertEqual(new_dist.low, 2)\n        self.assertEqual(new_dist.high, 8)\n        self.assertTrue(new_dist.log)\n        self.assertEqual(new_dist.step, 1)\n        self.assertNotEqual(new_dist, old_dist)\n\n        with self.assertWarns(FutureWarning):\n            _convert_old_distribution_to_new_distribution(old_dist, suppress_warning=False)\n\n    def test_categorical_distribution(self):\n        old_dist = CategoricalDistribution(choices=(1, 2, 3))\n        new_dist = _convert_old_distribution_to_new_distribution(old_dist)\n        self.assertIs(new_dist, old_dist)\n        self.assertEqual(new_dist, old_dist)\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            _convert_old_distribution_to_new_distribution(old_dist, suppress_warning=False)\n            self.assertEqual(len(w), 0)\n\n    def test_suppress_warning(self):\n        old_dist = UniformDistribution(low=1.0, high=2.0)\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            _convert_old_distribution_to_new_distribution(old_dist, suppress_warning=True)\n            self.assertEqual(len(w), 0)\n", "# Request\n    Generate tests, and necessary imports, only for the function '_is_distribution_log', part of the focal module whose code is provided below.\n    The focal module is called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" and belongs to a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the generated code will reside in the following path: \"distributions\"\n    - Suppose that the focal module resides in the following path: \"optuna\"\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        from contextlib import ContextDecorator, contextmanager\n\n        from django.db import (\n            DEFAULT_DB_ALIAS,\n            DatabaseError,\n            Error,\n            ProgrammingError,\n            connections,\n        )\n\n\n        class TransactionManagementError(ProgrammingError):\n            \"\"\"Transaction management is used improperly.\"\"\"\n\n            pass\n\n\n        def get_connection(using=None):\n            \"\"\"\n            Get a database connection by name, or the default database connection\n            if no name is provided. This is a private API.\n            \"\"\"\n            if using is None:\n                using = DEFAULT_DB_ALIAS\n            return connections[using]\n\n\n         def get_autocommit(using=None):\n            \"\"\"Get the autocommit status of the connection.\"\"\"\n            return get_connection(using).get_autocommit()\n\n\n        def set_autocommit(autocommit, using=None):\n            \"\"\"Set the autocommit status of the connection.\"\"\"\n            return get_connection(using).set_autocommit(autocommit)\n\n\n        def commit(using=None):\n            \"\"\"Commit a transaction.\"\"\"\n            get_connection(using).commit()\n\n\n        def rollback(using=None):\n            \"\"\"Roll back a transaction.\"\"\"\n            get_connection(using).rollback()\n\n\n        def savepoint(using=None):\n            \"\"\"\n            Create a savepoint (if supported and required by the backend) inside the\n            current transaction. Return an identifier for the savepoint that will be\n            used for the subsequent rollback or commit.\n            \"\"\"\n            return get_connection(using).savepoint()\n\n\n        def savepoint_rollback(sid, using=None):\n            \"\"\"\n            Roll back the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_rollback(sid)\n\n\n        def savepoint_commit(sid, using=None):\n            \"\"\"\n            Commit the most recent savepoint (if one exists). Do nothing if\n            savepoints are not supported.\n            \"\"\"\n            get_connection(using).savepoint_commit(sid)\n\n\n        def clean_savepoints(using=None):\n            \"\"\"\n            Reset the counter used to generate unique savepoint ids in this thread.\n            \"\"\"\n            get_connection(using).clean_savepoints()\n\n\n        def get_rollback(using=None):\n            \"\"\"Get the \"needs rollback\" flag -- for *advanced use* only.\"\"\"\n            return get_connection(using).get_rollback()\n\n\n        def set_rollback(rollback, using=None):\n            \"\"\"\n            Set or unset the \"needs rollback\" flag -- for *advanced use* only.\n\n            When `rollback` is `True`, trigger a rollback when exiting the innermost\n            enclosing atomic block that has `savepoint=True` (that's the default). Use\n            this to force a rollback without raising an exception.\n\n            When `rollback` is `False`, prevent such a rollback. Use this only after\n            rolling back to a known-good state! Otherwise, you break the atomic block\n            and data corruption may occur.\n            \"\"\"\n            return get_connection(using).set_rollback(rollback)\n\n\n        @contextmanager\n        def mark_for_rollback_on_error(using=None):\n            \"\"\"\n            Internal low-level utility to mark a transaction as \"needs rollback\" when\n            an exception is raised while not enforcing the enclosed block to be in a\n            transaction. This is needed by Model.save() and friends to avoid starting a\n            transaction when in autocommit mode and a single query is executed.\n\n            It's equivalent to:\n\n                connection = get_connection(using)\n                if connection.get_autocommit():\n                    yield\n                else:\n                    with transaction.atomic(using=using, savepoint=False):\n                        yield\n\n            but it uses low-level utilities to avoid performance overhead.\n            \"\"\"\n            try:\n                yield\n            except Exception as exc:\n                connection = get_connection(using)\n                if connection.in_atomic_block:\n                    connection.needs_rollback = True\n                    connection.rollback_exc = exc\n                raise\n\n\n        def on_commit(func, using=None, robust=False):\n            \"\"\"\n            Register `func` to be called when the current transaction is committed.\n            If the current transaction is rolled back, `func` will not be called.\n            \"\"\"\n            get_connection(using).on_commit(func, robust)\n\n\n        #################################\n        # Decorators / context managers #\n        #################################\n\n\n        class Atomic(ContextDecorator):\n            \"\"\"\n            Guarantee the atomic execution of a given block.\n\n            An instance can be used either as a decorator or as a context manager.\n\n            When it's used as a decorator, __call__ wraps the execution of the\n            decorated function in the instance itself, used as a context manager.\n\n            When it's used as a context manager, __enter__ creates a transaction or a\n            savepoint, depending on whether a transaction is already in progress, and\n            __exit__ commits the transaction or releases the savepoint on normal exit,\n            and rolls back the transaction or to the savepoint on exceptions.\n\n            It's possible to disable the creation of savepoints if the goal is to\n            ensure that some code runs within a transaction without creating overhead.\n\n            A stack of savepoint identifiers is maintained as an attribute of the\n            connection. None denotes the absence of a savepoint.\n\n            This allows reentrancy even if the same AtomicWrapper is reused. For\n            example, it's possible to define `oa = atomic('other')` and use `@oa` or\n            `with oa:` multiple times.\n\n            Since database connections are thread-local, this is thread-safe.\n\n            An atomic block can be tagged as durable. In this case, a RuntimeError is\n            raised if it's nested within another atomic block. This guarantees\n            that database changes in a durable block are committed to the database when\n            the block exits without error.\n\n            This is a private API.\n            \"\"\"\n\n            def __init__(self, using, savepoint, durable):\n                self.using = using\n                self.savepoint = savepoint\n                self.durable = durable\n                self._from_testcase = False\n\n            def __enter__(self):\n                connection = get_connection(self.using)\n\n                if (\n                    self.durable\n                    and connection.atomic_blocks\n                    and not connection.atomic_blocks[-1]._from_testcase\n                ):\n                    raise RuntimeError(\n                        \"A durable atomic block cannot be nested within another \"\n                        \"atomic block.\"\n                    )\n                if not connection.in_atomic_block:\n                    # Reset state when entering an outermost atomic block.\n                    connection.commit_on_exit = True\n                    connection.needs_rollback = False\n                    if not connection.get_autocommit():\n                        # Pretend we're already in an atomic block to bypass the code\n                        # that disables autocommit to enter a transaction, and make a\n                        # note to deal with this case in __exit__.\n                        connection.in_atomic_block = True\n                        connection.commit_on_exit = False\n\n                if connection.in_atomic_block:\n                    # We're already in a transaction; create a savepoint, unless we\n                    # were told not to or we're already waiting for a rollback. The\n                    # second condition avoids creating useless savepoints and prevents\n                    # overwriting needs_rollback until the rollback is performed.\n                    if self.savepoint and not connection.needs_rollback:\n                        sid = connection.savepoint()\n                        connection.savepoint_ids.append(sid)\n                    else:\n                        connection.savepoint_ids.append(None)\n                else:\n                    connection.set_autocommit(\n                        False, force_begin_transaction_with_broken_autocommit=True\n                    )\n                    connection.in_atomic_block = True\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.append(self)\n\n            def __exit__(self, exc_type, exc_value, traceback):\n                connection = get_connection(self.using)\n\n                if connection.in_atomic_block:\n                    connection.atomic_blocks.pop()\n\n                if connection.savepoint_ids:\n                    sid = connection.savepoint_ids.pop()\n                else:\n                    # Prematurely unset this flag to allow using commit or rollback.\n                    connection.in_atomic_block = False\n\n                try:\n                    if connection.closed_in_transaction:\n                        # The database will perform a rollback by itself.\n                        # Wait until we exit the outermost block.\n                        pass\n\n                    elif exc_type is None and not connection.needs_rollback:\n                        if connection.in_atomic_block:\n                            # Release savepoint if there is one\n                            if sid is not None:\n                                try:\n                                    connection.savepoint_commit(sid)\n                                except DatabaseError:\n                                    try:\n                                        connection.savepoint_rollback(sid)\n                                        # The savepoint won't be reused. Release it to\n                                        # minimize overhead for the database server.\n                                        connection.savepoint_commit(sid)\n                                    except Error:\n                                        # If rolling back to a savepoint fails, mark for\n                                        # rollback at a higher level and avoid shadowing\n                                        # the original exception.\n                                        connection.needs_rollback = True\n                                    raise\n                        else:\n                            # Commit transaction\n                            try:\n                                connection.commit()\n                            except DatabaseError:\n                                try:\n                                    connection.rollback()\n                                except Error:\n                                    # An error during rollback means that something\n                                    # went wrong with the connection. Drop it.\n                                    connection.close()\n                                raise\n                    else:\n                        # This flag will be set to True again if there isn't a savepoint\n                        # allowing to perform the rollback at this level.\n                        connection.needs_rollback = False\n                        if connection.in_atomic_block:\n                            # Roll back to savepoint if there is one, mark for rollback\n                            # otherwise.\n                            if sid is None:\n                                connection.needs_rollback = True\n                            else:\n                                try:\n                                    connection.savepoint_rollback(sid)\n                                    # The savepoint won't be reused. Release it to\n                                    # minimize overhead for the database server.\n                                    connection.savepoint_commit(sid)\n                                except Error:\n                                    # If rolling back to a savepoint fails, mark for\n                                    # rollback at a higher level and avoid shadowing\n                                    # the original exception.\n                                    connection.needs_rollback = True\n                        else:\n                            # Roll back transaction\n                            try:\n                                connection.rollback()\n                            except Error:\n                                # An error during rollback means that something\n                                # went wrong with the connection. Drop it.\n                                connection.close()\n\n                finally:\n                    # Outermost block exit when autocommit was enabled.\n                    if not connection.in_atomic_block:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.set_autocommit(True)\n                    # Outermost block exit when autocommit was disabled.\n                    elif not connection.savepoint_ids and not connection.commit_on_exit:\n                        if connection.closed_in_transaction:\n                            connection.connection = None\n                        else:\n                            connection.in_atomic_block = False\n\n\n        def atomic(using=None, savepoint=True, durable=False):\n            # Bare decorator: @atomic -- although the first argument is called\n            # `using`, it's actually the function being decorated.\n            if callable(using):\n                return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)\n            # Decorator: @atomic(...) or context manager: with atomic(...): ...\n            else:\n                return Atomic(using, savepoint, durable)\n\n\n        def _non_atomic_requests(view, using):\n            try:\n                view._non_atomic_requests.add(using)\n            except AttributeError:\n                view._non_atomic_requests = {using}\n            return view\n\n\n        def non_atomic_requests(using=None):\n            if callable(using):\n                return _non_atomic_requests(using, DEFAULT_DB_ALIAS)\n            else:\n                if using is None:\n                    using = DEFAULT_DB_ALIAS\n                return lambda view: _non_atomic_requests(view, using)\n        ```\n\n    ## Example Answer (only for the function 'mark_for_rollback_on_error'):\n        ```python\n        from unittest import TestCase\n\n        class ModuleFunctionsTests(TestCase):\n            available_apps = [\"transactions\"]\n\n            def test_mark_for_rollback_on_error(self):\n                with transaction.atomic(savepoint=False):\n                    # Swallow the intentional error raised.\n                    with self.assertRaisesMessage(Exception, \"Oops\"):\n                        # Wrap in `mark_for_rollback_on_error` to check if the\n                        # transaction is marked broken.\n                        with transaction.mark_for_rollback_on_error():\n                            # Ensure that we are still in a good state.\n                            self.assertFalse(transaction.get_rollback())\n\n                        # mark_for_rollback_on_error marked the transaction as broken \u00e2\u20ac\u00a6\n                        self.assertTrue(transaction.get_rollback())\n\n                    # \u00e2\u20ac\u00a6 and further queries fail.\n                    msg = \"You can't execute queries until the end of the 'atomic' block.\"\n                    with self.assertRaisesMessage(transaction.TransactionManagementError, msg):\n                        Reporter.objects.create()\n\n                # Transaction errors are reset at the end of an transaction, so this\n                # should just work.\n                Reporter.objects.create()\n\n                self.assertTrue(transaction.get_autocommit())\n                # Swallow the intentional error raised.\n                with self.assertRaisesMessage(Exception, \"Oops\"):\n                    # Wrap in `mark_for_rollback_on_error` to check if the transaction\n                    # is marked broken.\n                    with transaction.mark_for_rollback_on_error():\n                        # Ensure that we are still in a good state.\n                        self.assertFalse(transaction.get_connection().needs_rollback)\n\n                    # Ensure that `mark_for_rollback_on_error` did not mark the transaction\n                    # as broken, since we are in autocommit mode \u00e2\u20ac\u00a6\n                    self.assertFalse(transaction.get_connection().needs_rollback)\n\n                # \u00e2\u20ac\u00a6 and further queries work nicely.\n                Reporter.objects.create()\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the function '_is_distribution_log'):": "from unittest import TestCase\nfrom optuna.distributions import (\n    FloatDistribution,\n    IntDistribution,\n    CategoricalDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    IntUniformDistribution,\n    IntLogUniformDistribution,\n    _is_distribution_log,\n)\n\n\nclass TestIsDistributionLog(TestCase):\n    def test_float_distribution_log(self):\n        dist = FloatDistribution(1, 10, log=True)\n        self.assertTrue(_is_distribution_log(dist))\n        dist = FloatDistribution(1, 10, log=False)\n        self.assertFalse(_is_distribution_log(dist))\n\n    def test_int_distribution_log(self):\n        dist = IntDistribution(1, 10, log=True, step=1)\n        self.assertTrue(_is_distribution_log(dist))\n        dist = IntDistribution(1, 10, log=False, step=1)\n        self.assertFalse(_is_distribution_log(dist))\n\n    def test_categorical_distribution(self):\n        dist = CategoricalDistribution([1, 2, 3])\n        self.assertFalse(_is_distribution_log(dist))\n\n    def test_deprecated_distributions(self):\n        # Test deprecated Float distributions.\n        uniform_dist = UniformDistribution(1, 10)\n        self.assertFalse(_is_distribution_log(uniform_dist))\n        log_uniform_dist = LogUniformDistribution(1, 10)\n        self.assertTrue(_is_distribution_log(log_uniform_dist))\n        discrete_uniform_dist = DiscreteUniformDistribution(1, 10, 0.1)\n        self.assertFalse(_is_distribution_log(discrete_uniform_dist))\n\n        # Test deprecated Int distributions.\n        int_uniform_dist = IntUniformDistribution(1, 10, 1)\n        self.assertFalse(_is_distribution_log(int_uniform_dist))\n        int_log_uniform_dist = IntLogUniformDistribution(1, 10, 1)\n        self.assertTrue(_is_distribution_log(int_log_uniform_dist))\n", "# Request\n    Generate tests, and necessary imports, only for the method 'to_external_repr', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"BaseDistribution\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the method 'to_external_repr' of the class \"BaseDistribution\"):": "import unittest\nfrom distributions import BaseDistribution\n\nclass TestBaseDistribution(unittest.TestCase):\n    def test_to_external_repr(self):\n        class DummyDistribution(BaseDistribution):\n            def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n                return float(param_value_in_external_repr)\n            def single(self) -> bool:\n                return False\n            def _contains(self, param_value_in_internal_repr: float) -> bool:\n                return True\n\n        dist = DummyDistribution()\n        test_values = [3.14, 0.0, -5.5, 5.0, 1.23e-5]\n        for val in test_values:\n            with self.subTest(value=val):\n                self.assertEqual(dist.to_external_repr(val), val)\n", "# Request\n    Generate tests, and necessary imports, only for the method '_asdict', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"BaseDistribution\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the method '_asdict' of the class \"BaseDistribution\"):": "import unittest\nfrom typing import Any\n\nfrom distributions import BaseDistribution\n\n\nclass TestBaseDistribution(unittest.TestCase):\n    def test_asdict_returns_instance_attributes(self):\n        # Create a concrete subclass of BaseDistribution for testing\n        class ConcreteDistribution(BaseDistribution):\n            def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n                return 0.0\n\n            def single(self) -> bool:\n                return False\n\n            def _contains(self, param_value_in_internal_repr: float) -> bool:\n                return False\n\n        # Test case with multiple attributes\n        instance = ConcreteDistribution()\n        instance.attr1 = 123\n        instance.attr2 = \"test\"\n        instance.attr3 = [1, 2, 3]\n        self.assertEqual(instance._asdict(), {\"attr1\": 123, \"attr2\": \"test\", \"attr3\": [1, 2, 3]})\n\n    def test_asdict_with_constructor_set_attributes(self):\n        # Test case where attributes are set in __init__\n        class InitDistribution(BaseDistribution):\n            def __init__(self, x: int, y: str):\n                self.x = x\n                self.y = y\n\n            def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n                return 0.0\n\n            def single(self) -> bool:\n                return False\n\n            def _contains(self, param_value_in_internal_repr: float) -> bool:\n                return False\n\n        instance = InitDistribution(x=456, y=\"hello\")\n        self.assertEqual(instance._asdict(), {\"x\": 456, \"y\": \"hello\"})\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "# Request\n    Generate tests, and necessary imports, only for the method '__eq__', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"BaseDistribution\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the method '__eq__' of the class \"BaseDistribution\"):": "import unittest\n\nfrom distributions import BaseDistribution, FloatDistribution, IntDistribution, CategoricalDistribution\n\n\nclass TestBaseDistributionEq(unittest.TestCase):\n    def test_same_instance(self):\n        dist = FloatDistribution(0, 1)\n        self.assertEqual(dist, dist)\n\n    def test_same_type_same_attributes(self):\n        dist1 = FloatDistribution(0, 1, log=False, step=None)\n        dist2 = FloatDistribution(0, 1, log=False, step=None)\n        self.assertEqual(dist1, dist2)\n\n    def test_same_type_different_attributes(self):\n        dist1 = FloatDistribution(0, 1, log=False, step=None)\n        dist2 = FloatDistribution(0, 2, log=False, step=None)\n        self.assertNotEqual(dist1, dist2)\n\n    def test_different_type(self):\n        dist1 = FloatDistribution(0, 1)\n        dist2 = IntDistribution(0, 1)\n        self.assertNotEqual(dist1, dist2)\n\n    def test_not_base_distribution_instance(self):\n        dist = FloatDistribution(0, 1)\n        self.assertNotEqual(dist, object())\n\n    def test_custom_subclass_equality(self):\n        class MockDist(BaseDistribution):\n            def __init__(self, a, b):\n                self.a = a\n                self.b = b\n\n        dist1 = MockDist(1, 2)\n        dist2 = MockDist(1, 2)\n        self.assertEqual(dist1, dist2)\n\n        dist3 = MockDist(1, 3)\n        self.assertNotEqual(dist1, dist3)\n\n    def test_categorical_distribution_equality(self):\n        dist1 = CategoricalDistribution([1, 2, 3])\n        dist2 = CategoricalDistribution([1, 2, 3])\n        self.assertEqual(dist1, dist2)\n\n        dist3 = CategoricalDistribution([1, 2])\n        self.assertNotEqual(dist1, dist3)\n", "# Request\n    Generate tests, and necessary imports, only for the method '__hash__', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"BaseDistribution\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the method '__hash__' of the class \"BaseDistribution\"):": "import unittest\nfrom distributions import BaseDistribution\n\n\nclass TestBaseDistributionHash(unittest.TestCase):\n    def test_identical_instances_same_hash(self):\n        class DummyDist(BaseDistribution):\n            def to_internal_repr(self, x): return x\n            def single(self): return False\n            def _contains(self, x): return False\n\n        d1 = DummyDist(a=1, b=2)\n        d2 = DummyDist(a=1, b=2)\n        self.assertEqual(hash(d1), hash(d2))\n\n    def test_different_attributes_different_hash(self):\n        class DummyDist(BaseDistribution):\n            def to_internal_repr(self, x): return x\n            def single(self): return False\n            def _contains(self, x): return False\n\n        d1 = DummyDist(a=1, b=2)\n        d2 = DummyDist(a=1, b=3)\n        self.assertNotEqual(hash(d1), hash(d2))\n\n    def test_different_classes_different_hash(self):\n        class DummyDist1(BaseDistribution):\n            def to_internal_repr(self, x): return x\n            def single(self): return False\n            def _contains(self, x): return False\n\n        class DummyDist2(BaseDistribution):\n            def to_internal_repr(self, x): return x\n            def single(self): return False\n            def _contains(self, x): return False\n\n        d1 = DummyDist1(a=1, b=2)\n        d2 = DummyDist2(a=1, b=2)\n        self.assertNotEqual(hash(d1), hash(d2))\n\n    def test_attribute_order_does_not_affect_hash(self):\n        class DummyDist(BaseDistribution):\n            def to_internal_repr(self, x): return x\n            def single(self): return False\n            def _contains(self, x): return False\n\n        d1 = DummyDist(a=1, b=2)\n        d2 = DummyDist(b=2, a=1)\n        self.assertEqual(hash(d1), hash(d2))\n", "# Request\n    Generate tests, and necessary imports, only for the method '__repr__', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"BaseDistribution\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the method '__repr__' of the class \"BaseDistribution\"):": "from __future__ import annotations\n\nimport unittest\nfrom collections.abc import Sequence\n\nfrom distributions import BaseDistribution, CategoricalDistribution, FloatDistribution, UniformDistribution\n\n\nclass TestBaseDistributionRepr(unittest.TestCase):\n    def test_categorical_distribution_repr(self):\n        distribution = CategoricalDistribution([1, 2, 3])\n        expected = \"CategoricalDistribution(choices=(1, 2, 3))\"\n        self.assertEqual(repr(distribution), expected)\n\n    def test_uniform_distribution_repr(self):\n        distribution = UniformDistribution(low=1.0, high=2.0)\n        expected = \"UniformDistribution(high=2.0, low=1.0)\"\n        self.assertEqual(repr(distribution), expected)\n\n    def test_float_distribution_repr(self):\n        distribution = FloatDistribution(low=1.0, high=2.0, log=False, step=None)\n        expected = \"FloatDistribution(high=2.0, log=False, low=1.0, step=None)\"\n        self.assertEqual(repr(distribution), expected)\n\n    def test_float_distribution_with_step_repr(self):\n        distribution = FloatDistribution(low=1.0, high=3.0, log=False, step=0.5)\n        expected = \"FloatDistribution(high=3.0, log=False, low=1.0, step=0.5)\"\n        self.assertEqual(repr(distribution), expected)\n\n    def test_int_distribution_repr(self):\n        # Assuming IntDistribution is a subclass of BaseDistribution with similar structure\n        # For the purpose of this test, we'll mock it or use a real instance if available\n        # Since the actual IntDistribution code is provided, we can create an instance\n        from distributions import IntDistribution\n        distribution = IntDistribution(low=1, high=10, log=False, step=2)\n        expected = \"IntDistribution(high=10, log=False, low=1, step=2)\"\n        self.assertEqual(repr(distribution), expected)\n", "# Request\n    Generate tests, and necessary imports, only for the method '__init__', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"FloatDistribution\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the method '__init__' of the class \"FloatDistribution\"):": "import unittest\nimport warnings\nfrom distributions import FloatDistribution\n\n\nclass FloatDistributionTest(unittest.TestCase):\n    def test_init_valid_no_step_no_log(self):\n        dist = FloatDistribution(1.0, 2.0)\n        self.assertEqual(dist.low, 1.0)\n        self.assertEqual(dist.high, 2.0)\n        self.assertFalse(dist.log)\n        self.assertIsNone(dist.step)\n\n    def test_init_valid_with_step(self):\n        dist = FloatDistribution(0.0, 1.5, step=0.5)\n        self.assertEqual(dist.low, 0.0)\n        self.assertEqual(dist.high, 1.5)\n        self.assertFalse(dist.log)\n        self.assertEqual(dist.step, 0.5)\n\n    def test_init_log_and_step(self):\n        with self.assertRaises(ValueError):\n            FloatDistribution(1.0, 2.0, log=True, step=0.1)\n\n    def test_init_low_high_order(self):\n        with self.assertRaises(ValueError):\n            FloatDistribution(2.0, 1.0)\n\n    def test_init_log_low_nonpositive(self):\n        with self.assertRaises(ValueError):\n            FloatDistribution(0.0, 2.0, log=True)\n        with self.assertRaises(ValueError):\n            FloatDistribution(-1.0, 2.0, log=True)\n\n    def test_init_step_nonpositive(self):\n        with self.assertRaises(ValueError):\n            FloatDistribution(1.0, 2.0, step=0)\n        with self.assertRaises(ValueError):\n            FloatDistribution(1.0, 2.0, step=-0.5)\n\n    def test_init_step_adjust_high(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            dist = FloatDistribution(0.0, 1.6, step=0.5)\n            self.assertEqual(dist.high, 1.5)\n            self.assertEqual(len(w), 1)\n            self.assertIsInstance(w[0], warnings.WarningMessage)\n            self.assertIn(\"replaced by [0.0, 1.5]\", str(w[0].message))\n\n    def test_init_step_no_adjustment(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            dist = FloatDistribution(0.0, 1.5, step=0.5)\n            self.assertEqual(len(w), 0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "# Request\n    Generate tests, and necessary imports, only for the method 'single', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"FloatDistribution\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the method 'single' of the class \"FloatDistribution\"):": "import unittest\nfrom distributions import FloatDistribution\n\nclass TestFloatDistributionSingle(unittest.TestCase):\n    def test_single_no_step_equal_low_high(self):\n        dist = FloatDistribution(low=2.0, high=2.0)\n        self.assertTrue(dist.single())\n\n    def test_single_no_step_low_less_high(self):\n        dist = FloatDistribution(low=1.0, high=2.0)\n        self.assertFalse(dist.single())\n\n    def test_single_with_step_equal_low_high(self):\n        dist = FloatDistribution(low=3.0, high=3.0, step=0.5)\n        self.assertTrue(dist.single())\n\n    def test_single_with_step_adjusted_equal_low(self):\n        # Original high=0.05, step=0.1 \u2192 adjusted high=0.0\n        dist = FloatDistribution(low=0.0, high=0.05, step=0.1)\n        self.assertTrue(dist.single())\n\n    def test_single_with_step_equal_diff_step(self):\n        # low=0.0, high=0.3, step=0.3 \u2192 high - low == step \u2192 returns False\n        dist = FloatDistribution(low=0.0, high=0.3, step=0.3)\n        self.assertFalse(dist.single())\n\n    def test_single_with_step_greater_diff_step(self):\n        # low=0.0, original high=0.7, step=0.3 \u2192 adjusted high=0.6 \u2192 high - low > step\n        dist = FloatDistribution(low=0.0, high=0.7, step=0.3)\n        self.assertFalse(dist.single())\n", "# Request\n    Generate tests, and necessary imports, only for the method '_contains', part of the focal class, and focal module, whose code is provided below.\n    The focal class is called \"FloatDistribution\" and belongs to the module called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\gen_tests\\distributions\" of a project called \"C:\\Users\\filip\\Desktop\\Thesis_Repos\\optuna-master\\optuna-master\\optuna\\\".\n\n## Very Important\n    - Suppose that the focal module physically resides in the same directory of your generated code.\n    - You must follow the response format specified below for your answer.\n    - Do not modify any of the module's code to achieve the goal.\n    - Do not generate any code except for tests and necessary imports.\n\n# Output Format\n    ```python\n        <generated_code_here>\n    ```\n\n# Example:\n    ## Example Focal Module:\n        ```python\n        \"\"\"\n        Multi-part parsing for file uploads.\n\n        Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n        file upload handlers for processing.\n        \"\"\"\n\n        import base64\n        import binascii\n        import collections\n        import html\n\n        from django.http import QueryDict\n        from django.conf import settings\n        from django.core.exceptions import (\n            RequestDataTooBig,\n            SuspiciousMultipartForm,\n            TooManyFieldsSent,\n            TooManyFilesSent,\n        )\n        from django.core.files.uploadhandler import SkipFile, StopFutureHandlers, StopUpload\n        from django.utils.datastructures import MultiValueDict\n        from django.utils.encoding import force_str\n        from django.utils.http import parse_header_parameters\n        from django.utils.regex_helper import _lazy_re_compile\n\n        __all__ = (\"MultiPartParser\", \"MultiPartParserError\", \"InputStreamExhausted\")\n\n        class MultiPartParserError(Exception):\n            pass\n\n        class InputStreamExhausted(Exception):\n            \"\"\"\n            No more reads are allowed from this device.\n            \"\"\"\n\n            pass\n\n        RAW = \"raw\"\n        FILE = \"file\"\n        FIELD = \"field\"\n        FIELD_TYPES = frozenset([FIELD, RAW])\n        MAX_TOTAL_HEADER_SIZE = 1024\n\n        class MultiPartParser:\n            \"\"\"\n            An RFC 7578 multipart/form-data parser.\n\n            ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n            and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n            \"\"\"\n\n            boundary_re = _lazy_re_compile(r\"[ -~]{0,200}[!-~]\")\n\n            def __init__(self, META, input_data, upload_handlers, encoding=None):\n                \"\"\"\n                Initialize the MultiPartParser object.\n\n                :META:\n                    The standard ``META`` dictionary in Django request objects.\n                :input_data:\n                    The raw post data, as a file-like object.\n                :upload_handlers:\n                    A list of UploadHandler instances that perform operations on the\n                    uploaded data.\n                :encoding:\n                    The encoding with which to treat the incoming data.\n                \"\"\"\n                # Content-Type should contain multipart and the boundary information.\n                content_type = META.get(\"CONTENT_TYPE\", \"\")\n                if not content_type.startswith(\"multipart/\"):\n                    raise MultiPartParserError(\"Invalid Content-Type: %s\" % content_type)\n\n                try:\n                    content_type.encode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise MultiPartParserError(\n                        \"Invalid non-ASCII Content-Type in multipart: %s\"\n                        % force_str(content_type)\n                    )\n\n                # Parse the header to get the boundary to split the parts.\n                _, opts = parse_header_parameters(content_type)\n                boundary = opts.get(\"boundary\")\n                if not boundary or not self.boundary_re.fullmatch(boundary):\n                    raise MultiPartParserError(\n                        \"Invalid boundary in multipart: %s\" % force_str(boundary)\n                    )\n\n                # Content-Length should contain the length of the body we are about\n                # to receive.\n                try:\n                    content_length = int(META.get(\"CONTENT_LENGTH\", 0))\n                except (ValueError, TypeError):\n                    content_length = 0\n\n                if content_length < 0:\n                    # This means we shouldn't continue...raise an error.\n                    raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n\n                self._boundary = boundary.encode(\"ascii\")\n                self._input_data = input_data\n\n                # For compatibility with low-level network APIs (with 32-bit integers),\n                # the chunk size should be < 2^31, but still divisible by 4.\n                possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n                self._chunk_size = min([2**31 - 4, *possible_sizes])\n\n                self._meta = META\n                self._encoding = encoding or settings.DEFAULT_CHARSET\n                self._content_length = content_length\n                self._upload_handlers = upload_handlers\n\n            def parse(self):\n                # Call the actual parse routine and close all open files in case of\n                # errors. This is needed because if exceptions are thrown the\n                # MultiPartParser will not be garbage collected immediately and\n                # resources would be kept alive. This is only needed for errors because\n                # the Request object closes all uploaded files at the end of the\n                # request.\n                try:\n                    return self._parse()\n                except Exception:\n                    if hasattr(self, \"_files\"):\n                        for _, files in self._files.lists():\n                            for fileobj in files:\n                                fileobj.close()\n                    raise\n\n            def _parse(self):\n                \"\"\"\n                Parse the POST data and break it into a FILES MultiValueDict and a POST\n                MultiValueDict.\n\n                Return a tuple containing the POST and FILES dictionary, respectively.\n                \"\"\"\n\n                encoding = self._encoding\n                handlers = self._upload_handlers\n\n                # HTTP spec says that Content-Length >= 0 is valid\n                # handling content-length == 0 before continuing\n                if self._content_length == 0:\n                    return QueryDict(encoding=self._encoding), MultiValueDict()\n\n                # See if any of the handlers take care of the parsing.\n                # This allows overriding everything if need be.\n                for handler in handlers:\n                    result = handler.handle_raw_input(\n                        self._input_data,\n                        self._meta,\n                        self._content_length,\n                        self._boundary,\n                        encoding,\n                    )\n                    # Check to see if it was handled\n                    if result is not None:\n                        return result[0], result[1]\n\n                # Create the data structures to be used later.\n                self._post = QueryDict(mutable=True)\n                self._files = MultiValueDict()\n\n                # Instantiate the parser and stream:\n                stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n\n                # Whether or not to signal a file-completion at the beginning of the loop.\n                old_field_name = None\n                counters = [0] * len(handlers)\n\n                # Number of bytes that have been read.\n                num_bytes_read = 0\n                # To count the number of keys in the request.\n                num_post_keys = 0\n                # To count the number of files in the request.\n                num_files = 0\n                # To limit the amount of data read from the request.\n                read_size = None\n                # Whether a file upload is finished.\n                uploaded_file = True\n\n                try:\n                    for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n                        if old_field_name:\n                            # We run this at the beginning of the next loop\n                            # since we cannot be sure a file is complete until\n                            # we hit the next boundary/part of the multipart content.\n                            self.handle_file_complete(old_field_name, counters)\n                            old_field_name = None\n                            uploaded_file = True\n\n                        if (\n                            item_type in FIELD_TYPES\n                            and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None\n                        ):\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n                            num_post_keys += 1\n                            # 2 accounts for empty raw fields before and after the\n                            # last boundary.\n                            if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:\n                                raise TooManyFieldsSent(\n                                    \"The number of GET/POST parameters exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.\"\n                                )\n\n                        try:\n                            disposition = meta_data[\"content-disposition\"][1]\n                            field_name = disposition[\"name\"].strip()\n                        except (KeyError, IndexError, AttributeError):\n                            continue\n\n                        transfer_encoding = meta_data.get(\"content-transfer-encoding\")\n                        if transfer_encoding is not None:\n                            transfer_encoding = transfer_encoding[0].strip()\n                        field_name = force_str(field_name, encoding, errors=\"replace\")\n\n                        if item_type == FIELD:\n                            # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n                            if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n                                read_size = (\n                                    settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n                                )\n\n                            # This is a post field, we can just set it in the post\n                            if transfer_encoding == \"base64\":\n                                raw_data = field_stream.read(size=read_size)\n                                num_bytes_read += len(raw_data)\n                                try:\n                                    data = base64.b64decode(raw_data)\n                                except binascii.Error:\n                                    data = raw_data\n                            else:\n                                data = field_stream.read(size=read_size)\n                                num_bytes_read += len(data)\n\n                            # Add two here to make the check consistent with the\n                            # x-www-form-urlencoded check that includes '&='.\n                            num_bytes_read += len(field_name) + 2\n                            if (\n                                settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None\n                                and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE\n                            ):\n                                raise RequestDataTooBig(\n                                    \"Request body exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_MEMORY_SIZE.\"\n                                )\n\n                            self._post.appendlist(\n                                field_name, force_str(data, encoding, errors=\"replace\")\n                            )\n                        elif item_type == FILE:\n                            # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.\n                            num_files += 1\n                            if (\n                                settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None\n                                and num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES\n                            ):\n                                raise TooManyFilesSent(\n                                    \"The number of files exceeded \"\n                                    \"settings.DATA_UPLOAD_MAX_NUMBER_FILES.\"\n                                )\n                            # This is a file, use the handler...\n                            file_name = disposition.get(\"filename\")\n                            if file_name:\n                                file_name = force_str(file_name, encoding, errors=\"replace\")\n                                file_name = self.sanitize_file_name(file_name)\n                            if not file_name:\n                                continue\n\n                            content_type, content_type_extra = meta_data.get(\n                                \"content-type\", (\"\", {})\n                            )\n                            content_type = content_type.strip()\n                            charset = content_type_extra.get(\"charset\")\n\n                            try:\n                                content_length = int(meta_data.get(\"content-length\")[0])\n                            except (IndexError, TypeError, ValueError):\n                                content_length = None\n\n                            counters = [0] * len(handlers)\n                            uploaded_file = False\n                            try:\n                                for handler in handlers:\n                                    try:\n                                        handler.new_file(\n                                            field_name,\n                                            file_name,\n                                            content_type,\n                                            content_length,\n                                            charset,\n                                            content_type_extra,\n                                        )\n                                    except StopFutureHandlers:\n                                        break\n\n                                for chunk in field_stream:\n                                    if transfer_encoding == \"base64\":\n                                        # We only special-case base64 transfer encoding\n                                        # We should always decode base64 chunks by\n                                        # multiple of 4, ignoring whitespace.\n\n                                        stripped_chunk = b\"\".join(chunk.split())\n\n                                        remaining = len(stripped_chunk) % 4\n                                        while remaining != 0:\n                                            over_chunk = field_stream.read(4 - remaining)\n                                            if not over_chunk:\n                                                break\n                                            stripped_chunk += b\"\".join(over_chunk.split())\n                                            remaining = len(stripped_chunk) % 4\n\n                                        try:\n                                            chunk = base64.b64decode(stripped_chunk)\n                                        except Exception as exc:\n                                            # Since this is only a chunk, any error is\n                                            # an unfixable error.\n                                            raise MultiPartParserError(\n                                                \"Could not decode base64 data.\"\n                                            ) from exc\n\n                                    for i, handler in enumerate(handlers):\n                                        chunk_length = len(chunk)\n                                        chunk = handler.receive_data_chunk(chunk, counters[i])\n                                        counters[i] += chunk_length\n                                        if chunk is None:\n                                            # Don't continue if the chunk received by\n                                            # the handler is None.\n                                            break\n\n                            except SkipFile:\n                                self._close_files()\n                                # Just use up the rest of this file...\n                                exhaust(field_stream)\n                            else:\n                                # Handle file upload completions on next iteration.\n                                old_field_name = field_name\n                        else:\n                            # If this is neither a FIELD nor a FILE, exhaust the field\n                            # stream. Note: There could be an error here at some point,\n                            # but there will be at least two RAW types (before and\n                            # after the other boundaries). This branch is usually not\n                            # reached at all, because a missing content-disposition\n                            # header will skip the whole boundary.\n                            exhaust(field_stream)\n                except StopUpload as e:\n                    self._close_files()\n                    if not e.connection_reset:\n                        exhaust(self._input_data)\n                else:\n                    if not uploaded_file:\n                        for handler in handlers:\n                            handler.upload_interrupted()\n                    # Make sure that the request data is all fed\n                    exhaust(self._input_data)\n\n                # Signal that the upload has completed.\n                # any() shortcircuits if a handler's upload_complete() returns a value.\n                any(handler.upload_complete() for handler in handlers)\n                self._post._mutable = False\n                return self._post, self._files\n\n            def handle_file_complete(self, old_field_name, counters):\n                \"\"\"\n                Handle all the signaling that takes place when a file is complete.\n                \"\"\"\n                for i, handler in enumerate(self._upload_handlers):\n                    file_obj = handler.file_complete(counters[i])\n                    if file_obj:\n                        # If it returns a file object, then set the files dict.\n                        self._files.appendlist(\n                            force_str(old_field_name, self._encoding, errors=\"replace\"),\n                            file_obj,\n                        )\n                        break\n\n            def sanitize_file_name(self, file_name):\n                \"\"\"\n                Sanitize the filename of an upload.\n\n                Remove all possible path separators, even though that might remove more\n                than actually required by the target system. Filenames that could\n                potentially cause problems (current/parent dir) are also discarded.\n\n                It should be noted that this function could still return a \"filepath\"\n                like \"C:some_file.txt\" which is handled later on by the storage layer.\n                So while this function does sanitize filenames to some extent, the\n                resulting filename should still be considered as untrusted user input.\n                \"\"\"\n                file_name = html.unescape(file_name)\n                file_name = file_name.rsplit(\"/\")[-1]\n                file_name = file_name.rsplit(\"\\\\\")[-1]\n                # Remove non-printable characters.\n                file_name = \"\".join([char for char in file_name if char.isprintable()])\n\n                if file_name in {\"\", \".\", \"..\"}:\n                    return None\n                return file_name\n\n            IE_sanitize = sanitize_file_name\n\n            def _close_files(self):\n                # Free up all file handles.\n                # FIXME: this currently assumes that upload handlers store the file as 'file'\n                # We should document that...\n                # (Maybe add handler.free_file to complement new_file)\n                for handler in self._upload_handlers:\n                    if hasattr(handler, \"file\"):\n                        handler.file.close()\n\n        class LazyStream:\n            \"\"\"\n            The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n            Given a producer object (an iterator that yields bytestrings), the\n            LazyStream object will support iteration, reading, and keeping a \"look-back\"\n            variable in case you need to \"unget\" some bytes.\n            \"\"\"\n\n            def __init__(self, producer, length=None):\n                \"\"\"\n                Every LazyStream must have a producer when instantiated.\n\n                A producer is an iterable that returns a string each time it\n                is called.\n                \"\"\"\n                self._producer = producer\n                self._empty = False\n                self._leftover = b\"\"\n                self.length = length\n                self.position = 0\n                self._remaining = length\n                self._unget_history = []\n\n            def tell(self):\n                return self.position\n\n            def read(self, size=None):\n                def parts():\n                    remaining = self._remaining if size is None else size\n                    # do the whole thing in one shot if no limit was provided.\n                    if remaining is None:\n                        yield b\"\".join(self)\n                        return\n\n                    # otherwise do some bookkeeping to return exactly enough\n                    # of the stream and stashing any extra content we get from\n                    # the producer\n                    while remaining != 0:\n                        assert remaining > 0, \"remaining bytes to read should never go negative\"\n\n                        try:\n                            chunk = next(self)\n                        except StopIteration:\n                            return\n                        else:\n                            emitting = chunk[:remaining]\n                            self.unget(chunk[remaining:])\n                            remaining -= len(emitting)\n                            yield emitting\n\n                return b\"\".join(parts())\n\n            def __next__(self):\n                \"\"\"\n                Used when the exact number of bytes to read is unimportant.\n\n                Return whatever chunk is conveniently returned from the iterator.\n                Useful to avoid unnecessary bookkeeping if performance is an issue.\n                \"\"\"\n                if self._leftover:\n                    output = self._leftover\n                    self._leftover = b\"\"\n                else:\n                    output = next(self._producer)\n                    self._unget_history = []\n                self.position += len(output)\n                return output\n\n            def close(self):\n                \"\"\"\n                Used to invalidate/disable this lazy stream.\n\n                Replace the producer with an empty list. Any leftover bytes that have\n                already been read will still be reported upon read() and/or next().\n                \"\"\"\n                self._producer = []\n\n            def __iter__(self):\n                return self\n\n            def unget(self, bytes):\n                \"\"\"\n                Place bytes back onto the front of the lazy stream.\n\n                Future calls to read() will return those bytes first. The\n                stream position and thus tell() will be rewound.\n                \"\"\"\n                if not bytes:\n                    return\n                self._update_unget_history(len(bytes))\n                self.position -= len(bytes)\n                self._leftover = bytes + self._leftover\n\n            def _update_unget_history(self, num_bytes):\n                \"\"\"\n                Update the unget history as a sanity check to see if we've pushed\n                back the same number of bytes in one chunk. If we keep ungetting the\n                same number of bytes many times (here, 50), we're mostly likely in an\n                infinite loop of some sort. This is usually caused by a\n                maliciously-malformed MIME request.\n                \"\"\"\n                self._unget_history = [num_bytes] + self._unget_history[:49]\n                number_equal = len(\n                    [\n                        current_number\n                        for current_number in self._unget_history\n                        if current_number == num_bytes\n                    ]\n                )\n\n                if number_equal > 40:\n                    raise SuspiciousMultipartForm(\n                        \"The multipart parser got stuck, which shouldn't happen with\"\n                        \" normal uploaded files. Check for malicious upload activity;\"\n                        \" if there is none, report this to the Django developers.\"\n                    )\n\n        class ChunkIter:\n            \"\"\"\n            An iterable that will yield chunks of data. Given a file-like object as the\n            constructor, yield chunks of read operations from that object.\n            \"\"\"\n\n            def __init__(self, flo, chunk_size=64 * 1024):\n                self.flo = flo\n                self.chunk_size = chunk_size\n\n            def __next__(self):\n                try:\n                    data = self.flo.read(self.chunk_size)\n                except InputStreamExhausted:\n                    raise StopIteration()\n                if data:\n                    return data\n                else:\n                    raise StopIteration()\n\n            def __iter__(self):\n                return self\n\n        class InterBoundaryIter:\n            \"\"\"\n            A Producer that will iterate over boundaries.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                try:\n                    return LazyStream(BoundaryIter(self._stream, self._boundary))\n                except InputStreamExhausted:\n                    raise StopIteration()\n\n        class BoundaryIter:\n            \"\"\"\n            A Producer that is sensitive to boundaries.\n\n            Will happily yield bytes until a boundary is found. Will yield the bytes\n            before the boundary, throw away the boundary bytes themselves, and push the\n            post-boundary bytes back on the stream.\n\n            The future calls to next() after locating the boundary will raise a\n            StopIteration exception.\n            \"\"\"\n\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._boundary = boundary\n                self._done = False\n                # rollback an additional six bytes because the format is like\n                # this: CRLF<boundary>[--CRLF]\n                self._rollback = len(boundary) + 6\n\n                # Try to use mx fast string search if available. Otherwise\n                # use Python find. Wrap the latter for consistency.\n                unused_char = self._stream.read(1)\n                if not unused_char:\n                    raise InputStreamExhausted()\n                self._stream.unget(unused_char)\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self._done:\n                    raise StopIteration()\n\n                stream = self._stream\n                rollback = self._rollback\n\n                bytes_read = 0\n                chunks = []\n                for bytes in stream:\n                    bytes_read += len(bytes)\n                    chunks.append(bytes)\n                    if bytes_read > rollback:\n                        break\n                    if not bytes:\n                        break\n                else:\n                    self._done = True\n\n                if not chunks:\n                    raise StopIteration()\n\n                chunk = b\"\".join(chunks)\n                boundary = self._find_boundary(chunk)\n\n                if boundary:\n                    end, next = boundary\n                    stream.unget(chunk[next:])\n                    self._done = True\n                    return chunk[:end]\n                else:\n                    # make sure we don't treat a partial boundary (and\n                    # its separators) as data\n                    if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n                        # There's nothing left, we should just return and mark as done.\n                        self._done = True\n                        return chunk\n                    else:\n                        stream.unget(chunk[-rollback:])\n                        return chunk[:-rollback]\n\n            def _find_boundary(self, data):\n                \"\"\"\n                Find a multipart boundary in data.\n\n                Should no boundary exist in the data, return None. Otherwise, return\n                a tuple containing the indices of the following:\n                 * the end of current encapsulation\n                 * the start of the next encapsulation\n                \"\"\"\n                index = data.find(self._boundary)\n                if index < 0:\n                    return None\n                else:\n                    end = index\n                    next = index + len(self._boundary)\n                    # backup over CRLF\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\n\":\n                        end -= 1\n                    last = max(0, end - 1)\n                    if data[last : last + 1] == b\"\\r\":\n                        end -= 1\n                    return end, next\n\n        def exhaust(stream_or_iterable):\n            \"\"\"Exhaust an iterator or stream.\"\"\"\n            try:\n                iterator = iter(stream_or_iterable)\n            except TypeError:\n                iterator = ChunkIter(stream_or_iterable, 16384)\n            collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n\n        def parse_boundary_stream(stream, max_header_size):\n            \"\"\"\n            Parse one and exactly one stream that encapsulates a boundary.\n            \"\"\"\n\n            # Look for the end of headers and if not found extend the search to double\n            # the size up to the MAX_TOTAL_HEADER_SIZE.\n            headers_chunk_size = 1024\n            while True:\n                if headers_chunk_size > max_header_size:\n                    raise MultiPartParserError(\"Request max total header size exceeded.\")\n\n                # Stream at beginning of header, look for end of header and parse it if\n                # found. The header must fit within one chunk.\n                chunk = stream.read(headers_chunk_size)\n                # 'find' returns the top of these four bytes, so munch them later to\n                # prevent them from polluting the payload.\n                header_end = chunk.find(b\"\\r\\n\\r\\n\")\n                if header_end != -1:\n                    break\n\n                # Find no header, mark this fact and pass on the stream verbatim.\n                stream.unget(chunk)\n                # No more data to read.\n                if len(chunk) < headers_chunk_size:\n                    return (RAW, {}, stream)\n                # Double the chunk size.\n                headers_chunk_size *= 2\n\n            header = chunk[:header_end]\n\n            # here we place any excess chunk back onto the stream, as\n            # well as throwing away the CRLFCRLF bytes from above.\n            stream.unget(chunk[header_end + 4 :])\n\n            TYPE = RAW\n            outdict = {}\n\n            # Eliminate blank lines\n            for line in header.split(b\"\\r\\n\"):\n                # This terminology (\"main value\" and \"dictionary of\n                # parameters\") is from the Python docs.\n                try:\n                    main_value_pair, params = parse_header_parameters(line.decode())\n                    name, value = main_value_pair.split(\":\", 1)\n                    params = {k: v.encode() for k, v in params.items()}\n                except ValueError:  # Invalid header.\n                    continue\n\n                if name == \"content-disposition\":\n                    TYPE = FIELD\n                    if params.get(\"filename\"):\n                        TYPE = FILE\n\n                outdict[name] = value, params\n\n            if TYPE == RAW:\n                stream.unget(chunk)\n\n            return (TYPE, outdict, stream)\n\n\n        class Parser:\n            def __init__(self, stream, boundary):\n                self._stream = stream\n                self._separator = b\"--\" + boundary\n\n            def __iter__(self):\n                boundarystream = InterBoundaryIter(self._stream, self._separator)\n                for sub_stream in boundarystream:\n                    # Iterate over each part\n                    yield parse_boundary_stream(sub_stream, MAX_TOTAL_HEADER_SIZE)\n        ```\n\n    ## Example Answer (only for the method 'sanitize_file_name' of the class \"MultiPartParser\"):\n        ```python\n        import base64\n        import hashlib\n        import os\n        import shutil\n        import sys\n        import tempfile as sys_tempfile\n        import unittest\n        from io import BytesIO, StringIO\n        from unittest import mock\n        from urllib.parse import quote\n\n        from django.conf import DEFAULT_STORAGE_ALIAS\n        from django.core.exceptions import SuspiciousFileOperation\n        from django.core.files import temp as tempfile\n        from django.core.files.storage import default_storage\n        from django.core.files.uploadedfile import SimpleUploadedFile, UploadedFile\n        from django.http.multipartparser import (\n            FILE,\n            MAX_TOTAL_HEADER_SIZE,\n            MultiPartParser,\n            MultiPartParserError,\n            Parser,\n        )\n        from django.test import SimpleTestCase, TestCase, client, override_settings\n\n        from . import uploadhandler\n        from .models import FileModel\n\n        UNICODE_FILENAME = \"test-0123456789_\u00e4\u00b8\u00ad\u00e6\u2013\u2021_Orl\u00c3\u00a9ans.jpg\"\n        MEDIA_ROOT = sys_tempfile.mkdtemp()\n        UPLOAD_FOLDER = \"test_upload\"\n        UPLOAD_TO = os.path.join(MEDIA_ROOT, UPLOAD_FOLDER)\n\n        CANDIDATE_TRAVERSAL_FILE_NAMES = [\n            \"/tmp/hax0rd.txt\",  # Absolute path, *nix-style.\n            \"C:\\\\Windows\\\\hax0rd.txt\",  # Absolute path, win-style.\n            \"C:/Windows/hax0rd.txt\",  # Absolute path, broken-style.\n            \"\\\\tmp\\\\hax0rd.txt\",  # Absolute path, broken in a different way.\n            \"/tmp\\\\hax0rd.txt\",  # Absolute path, broken by mixing.\n            \"subdir/hax0rd.txt\",  # Descendant path, *nix-style.\n            \"subdir\\\\hax0rd.txt\",  # Descendant path, win-style.\n            \"sub/dir\\\\hax0rd.txt\",  # Descendant path, mixed.\n            \"../../hax0rd.txt\",  # Relative path, *nix-style.\n            \"..\\\\..\\\\hax0rd.txt\",  # Relative path, win-style.\n            \"../..\\\\hax0rd.txt\",  # Relative path, mixed.\n            \"..&#x2F;hax0rd.txt\",  # HTML entities.\n            \"..&sol;hax0rd.txt\",  # HTML entities.\n        ]\n\n        CANDIDATE_INVALID_FILE_NAMES = [\n            \"/tmp/\",  # Directory, *nix-style.\n            \"c:\\\\tmp\\\\\",  # Directory, win-style.\n            \"/tmp/.\",  # Directory dot, *nix-style.\n            \"c:\\\\tmp\\\\.\",  # Directory dot, *nix-style.\n            \"/tmp/..\",  # Parent directory, *nix-style.\n            \"c:\\\\tmp\\\\..\",  # Parent directory, win-style.\n            \"\",  # Empty filename.\n        ]\n\n        class MultiPartParserTests(SimpleTestCase):\n            def test_sanitize_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_TRAVERSAL_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertEqual(parser.sanitize_file_name(file_name), \"hax0rd.txt\")\n\n            def test_sanitize_invalid_file_name(self):\n                parser = MultiPartParser(\n                    {\n                        \"CONTENT_TYPE\": \"multipart/form-data; boundary=_foo\",\n                        \"CONTENT_LENGTH\": \"1\",\n                    },\n                    StringIO(\"x\"),\n                    [],\n                    \"utf-8\",\n                )\n                for file_name in CANDIDATE_INVALID_FILE_NAMES:\n                    with self.subTest(file_name=file_name):\n                        self.assertIsNone(parser.sanitize_file_name(file_name))\n        ```\n\n# Focal module:\n```python\nfrom __future__ import annotations\n\nimport abc\nfrom collections.abc import Sequence\nimport copy\nimport decimal\nimport json\nimport math\nfrom numbers import Real\nfrom typing import Any\nfrom typing import cast\nfrom typing import Union\nimport warnings\n\nfrom optuna._deprecated import deprecated_class\n\n\nCategoricalChoiceType = Union[None, bool, int, float, str]\n\n\n_float_distribution_deprecated_msg = (\n    \"Use :class:`~optuna.distributions.FloatDistribution` instead.\"\n)\n_int_distribution_deprecated_msg = \"Use :class:`~optuna.distributions.IntDistribution` instead.\"\n\n\nclass BaseDistribution(abc.ABC):\n    \"\"\"Base class for distributions.\n\n    Note that distribution classes are not supposed to be called by library users.\n    They are used by :class:`~optuna.trial.Trial` and :class:`~optuna.samplers` internally.\n    \"\"\"\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> Any:\n        \"\"\"Convert internal representation of a parameter value into external representation.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            Optuna's external representation of a parameter value.\n        \"\"\"\n\n        return param_value_in_internal_repr\n\n    @abc.abstractmethod\n    def to_internal_repr(self, param_value_in_external_repr: Any) -> float:\n        \"\"\"Convert external representation of a parameter value into internal representation.\n\n        Args:\n            param_value_in_external_repr:\n                Optuna's external representation of a parameter value.\n\n        Returns:\n            Optuna's internal representation of a parameter value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def single(self) -> bool:\n        \"\"\"Test whether the range of this distribution contains just a single value.\n\n        Returns:\n            :obj:`True` if the range of this distribution contains just a single value,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        \"\"\"Test if a parameter value is contained in the range of this distribution.\n\n        Args:\n            param_value_in_internal_repr:\n                Optuna's internal representation of a parameter value.\n\n        Returns:\n            :obj:`True` if the parameter value is contained in the range of this distribution,\n            otherwise :obj:`False`.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def _asdict(self) -> dict:\n        return self.__dict__\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if type(self) is not type(other):\n            return False\n        return self.__dict__ == other.__dict__\n\n    def __hash__(self) -> int:\n        return hash((self.__class__,) + tuple(sorted(self.__dict__.items())))\n\n    def __repr__(self) -> str:\n        kwargs = \", \".join(\"{}={}\".format(k, v) for k, v in sorted(self._asdict().items()))\n        return \"{}({})\".format(self.__class__.__name__, kwargs)\n\n\nclass FloatDistribution(BaseDistribution):\n    \"\"\"A distribution on floats.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than 0.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not :obj:`None`.\n        step:\n            A discretization step. ``step`` must be larger than 0.\n            This parameter must be :obj:`None` when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(\n        self, low: float, high: float, log: bool = False, step: None | float = None\n    ) -> None:\n        if log and step is not None:\n            raise ValueError(\"The parameter `step` is not supported when `log` is true.\")\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low <= 0.0:\n            raise ValueError(\n                \"The `low` value must be larger than 0 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step is not None and step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, \" \"but step={}.\".format(step)\n            )\n\n        self.step = None\n        if step is not None:\n            high = _adjust_discrete_uniform_high(low, high, step)\n            self.step = float(step)\n\n        self.low = float(low)\n        self.high = float(high)\n        self.log = log\n\n    def single(self) -> bool:\n        if self.step is None:\n            return self.low == self.high\n        else:\n            if self.low == self.high:\n                return True\n            high = decimal.Decimal(str(self.high))\n            low = decimal.Decimal(str(self.low))\n            step = decimal.Decimal(str(self.step))\n            return (high - low) < step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        if self.step is None:\n            return self.low <= value <= self.high\n        else:\n            k = (value - self.low) / self.step\n            return self.low <= value <= self.high and abs(k - round(k)) < 1.0e-8\n\n    def to_internal_repr(self, param_value_in_external_repr: float) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass UniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=False, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass LogUniformDistribution(FloatDistribution):\n    \"\"\"A uniform distribution in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``log=True``,\n    and passed to :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be larger than 0. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float) -> None:\n        super().__init__(low=low, high=high, log=True, step=None)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        d.pop(\"step\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_float_distribution_deprecated_msg)\nclass DiscreteUniformDistribution(FloatDistribution):\n    \"\"\"A discretized uniform distribution in the linear domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_float` with ``step``\n    argument, and passed to :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by :math:`q`,\n        :math:`\\\\mathsf{high}` will be replaced with the maximum of :math:`k q + \\\\mathsf{low}\n        < \\\\mathsf{high}`, where :math:`k` is an integer.\n\n    Args:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        q:\n            A discretization step. ``q`` must be larger than 0.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n\n    \"\"\"\n\n    def __init__(self, low: float, high: float, q: float) -> None:\n        super().__init__(low=low, high=high, step=q)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n\n        step = d.pop(\"step\")\n        d[\"q\"] = step\n        return d\n\n    @property\n    def q(self) -> float:\n        \"\"\"Discretization step.\n\n        :class:`~optuna.distributions.DiscreteUniformDistribution` is a subtype of\n        :class:`~optuna.distributions.FloatDistribution`.\n        This property is a proxy for its ``step`` attribute.\n        \"\"\"\n        return cast(float, self.step)\n\n    @q.setter\n    def q(self, v: float) -> None:\n        self.step = v\n\n\nclass IntDistribution(BaseDistribution):\n    \"\"\"A distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        When ``step`` is not :obj:`None`, if the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]`\n        is not divisible by :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced\n        with the maximum of :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`,\n        where :math:`k` is an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``. If ``log`` is :obj:`True`,\n            ``low`` must be larger than or equal to 1.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        log:\n            If ``log`` is :obj:`True`, this distribution is in log-scaled domain.\n            In this case, all parameters enqueued to the distribution must be positive values.\n            This parameter must be :obj:`False` when the parameter ``step`` is not 1.\n        step:\n            A discretization step. ``step`` must be a positive integer. This parameter must be 1\n            when the parameter ``log`` is :obj:`True`.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, log: bool = False, step: int = 1) -> None:\n        if log and step != 1:\n            raise ValueError(\n                \"Samplers and other components in Optuna only accept step is 1 \"\n                \"when `log` argument is True.\"\n            )\n\n        if low > high:\n            raise ValueError(\n                \"The `low` value must be smaller than or equal to the `high` value \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if log and low < 1:\n            raise ValueError(\n                \"The `low` value must be equal to or greater than 1 for a log distribution \"\n                \"(low={}, high={}).\".format(low, high)\n            )\n\n        if step <= 0:\n            raise ValueError(\n                \"The `step` value must be non-zero positive value, but step={}.\".format(step)\n            )\n\n        self.log = log\n        self.step = int(step)\n        self.low = int(low)\n        high = int(high)\n        self.high = _adjust_int_uniform_high(self.low, high, self.step)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> int:\n        return int(param_value_in_internal_repr)\n\n    def to_internal_repr(self, param_value_in_external_repr: int) -> float:\n        try:\n            internal_repr = float(param_value_in_external_repr)\n        except (ValueError, TypeError) as e:\n            raise ValueError(\n                f\"'{param_value_in_external_repr}' is not a valid type. \"\n                \"float-castable value is expected.\"\n            ) from e\n\n        if math.isnan(internal_repr):\n            raise ValueError(f\"`{param_value_in_external_repr}` is invalid value.\")\n        if self.log and internal_repr <= 0.0:\n            raise ValueError(\n                f\"`{param_value_in_external_repr}` is invalid value for the case log=True.\"\n            )\n        return internal_repr\n\n    def single(self) -> bool:\n        if self.log:\n            return self.low == self.high\n\n        if self.low == self.high:\n            return True\n        return (self.high - self.low) < self.step\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        value = param_value_in_internal_repr\n        return self.low <= value <= self.high and (value - self.low) % self.step == 0\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    .. note::\n        If the range :math:`[\\\\mathsf{low}, \\\\mathsf{high}]` is not divisible by\n        :math:`\\\\mathsf{step}`, :math:`\\\\mathsf{high}` will be replaced with the maximum of\n        :math:`k \\\\times \\\\mathsf{step} + \\\\mathsf{low} < \\\\mathsf{high}`, where :math:`k` is\n        an integer.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range.\n            ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=False, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\n@deprecated_class(\"3.0.0\", \"6.0.0\", text=_int_distribution_deprecated_msg)\nclass IntLogUniformDistribution(IntDistribution):\n    \"\"\"A uniform distribution on integers in the log domain.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_int`, and passed to\n    :mod:`~optuna.samplers` in general.\n\n    Attributes:\n        low:\n            Lower endpoint of the range of the distribution. ``low`` is included in the range\n            and must be larger than or equal to 1. ``low`` must be less than or equal to ``high``.\n        high:\n            Upper endpoint of the range of the distribution. ``high`` is included in the range.\n            ``high`` must be greater than or equal to ``low``.\n        step:\n            A discretization step. ``step`` must be a positive integer.\n\n    \"\"\"\n\n    def __init__(self, low: int, high: int, step: int = 1) -> None:\n        super().__init__(low=low, high=high, log=True, step=step)\n\n    def _asdict(self) -> dict:\n        d = copy.deepcopy(self.__dict__)\n        d.pop(\"log\")\n        return d\n\n\ndef _categorical_choice_equal(\n    value1: CategoricalChoiceType, value2: CategoricalChoiceType\n) -> bool:\n    \"\"\"A function to check two choices equal considering NaN.\n\n    This function can handle NaNs like np.float32(\"nan\") other than float.\n    \"\"\"\n\n    value1_is_nan = isinstance(value1, Real) and math.isnan(float(value1))\n    value2_is_nan = isinstance(value2, Real) and math.isnan(float(value2))\n    return (value1 == value2) or (value1_is_nan and value2_is_nan)\n\n\nclass CategoricalDistribution(BaseDistribution):\n    \"\"\"A categorical distribution.\n\n    This object is instantiated by :func:`~optuna.trial.Trial.suggest_categorical`, and\n    passed to :mod:`~optuna.samplers` in general.\n\n    Args:\n        choices:\n            Parameter value candidates. ``choices`` must have one element at least.\n\n    .. note::\n\n        Not all types are guaranteed to be compatible with all storages. It is recommended to\n        restrict the types of the choices to :obj:`None`, :class:`bool`, :class:`int`,\n        :class:`float` and :class:`str`.\n\n    Attributes:\n        choices:\n            Parameter value candidates.\n\n    \"\"\"\n\n    def __init__(self, choices: Sequence[CategoricalChoiceType]) -> None:\n        if len(choices) == 0:\n            raise ValueError(\"The `choices` must contain one or more elements.\")\n        for choice in choices:\n            if choice is not None and not isinstance(choice, (bool, int, float, str)):\n                message = (\n                    \"Choices for a categorical distribution should be a tuple of None, bool, \"\n                    \"int, float and str for persistent storage but contains {} which is of type \"\n                    \"{}.\".format(choice, type(choice).__name__)\n                )\n                warnings.warn(message)\n\n        self.choices = tuple(choices)\n\n    def to_external_repr(self, param_value_in_internal_repr: float) -> CategoricalChoiceType:\n        return self.choices[int(param_value_in_internal_repr)]\n\n    def to_internal_repr(self, param_value_in_external_repr: CategoricalChoiceType) -> float:\n        try:\n            # NOTE(nabenabe): With this implementation, we cannot distinguish some values\n            # such as True and 1, or 1.0 and 1. For example, if choices=[True, 1] and external_repr\n            # is 1, this method wrongly returns 0 instead of 1. However, we decided to accept this\n            # bug for such exceptional choices for less complexity and faster processing.\n            return self.choices.index(param_value_in_external_repr)\n        except ValueError:  # ValueError: param_value_in_external_repr is not in choices.\n            # ValueError also happens if external_repr is nan or includes precision error in float.\n            for index, choice in enumerate(self.choices):\n                if _categorical_choice_equal(param_value_in_external_repr, choice):\n                    return index\n\n        raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n\n    def single(self) -> bool:\n        return len(self.choices) == 1\n\n    def _contains(self, param_value_in_internal_repr: float) -> bool:\n        index = int(param_value_in_internal_repr)\n        return 0 <= index < len(self.choices)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, BaseDistribution):\n            return NotImplemented\n        if not isinstance(other, self.__class__):\n            return False\n        if self.__dict__.keys() != other.__dict__.keys():\n            return False\n        for key, value in self.__dict__.items():\n            if key == \"choices\":\n                if len(value) != len(getattr(other, key)):\n                    return False\n                for choice, other_choice in zip(value, getattr(other, key)):\n                    if not _categorical_choice_equal(choice, other_choice):\n                        return False\n            else:\n                if value != getattr(other, key):\n                    return False\n        return True\n\n    __hash__ = BaseDistribution.__hash__\n\n\nDISTRIBUTION_CLASSES = (\n    IntDistribution,\n    IntLogUniformDistribution,\n    IntUniformDistribution,\n    FloatDistribution,\n    UniformDistribution,\n    LogUniformDistribution,\n    DiscreteUniformDistribution,\n    CategoricalDistribution,\n)\n\n\ndef json_to_distribution(json_str: str) -> BaseDistribution:\n    \"\"\"Deserialize a distribution in JSON format.\n\n    Args:\n        json_str: A JSON-serialized distribution.\n\n    Returns:\n        A deserialized distribution.\n\n    \"\"\"\n\n    json_dict = json.loads(json_str)\n\n    if \"name\" in json_dict:\n        if json_dict[\"name\"] == CategoricalDistribution.__name__:\n            json_dict[\"attributes\"][\"choices\"] = tuple(json_dict[\"attributes\"][\"choices\"])\n\n        for cls in DISTRIBUTION_CLASSES:\n            if json_dict[\"name\"] == cls.__name__:\n                return cls(**json_dict[\"attributes\"])\n\n        raise ValueError(\"Unknown distribution class: {}\".format(json_dict[\"name\"]))\n\n    else:\n        # Deserialize a distribution from an abbreviated format.\n        if json_dict[\"type\"] == \"categorical\":\n            return CategoricalDistribution(json_dict[\"choices\"])\n        elif json_dict[\"type\"] in (\"float\", \"int\"):\n            low = json_dict[\"low\"]\n            high = json_dict[\"high\"]\n            step = json_dict.get(\"step\")\n            log = json_dict.get(\"log\", False)\n\n            if json_dict[\"type\"] == \"float\":\n                return FloatDistribution(low, high, log=log, step=step)\n\n            else:\n                if step is None:\n                    step = 1\n                return IntDistribution(low=low, high=high, log=log, step=step)\n\n        raise ValueError(\"Unknown distribution type: {}\".format(json_dict[\"type\"]))\n\n\ndef distribution_to_json(dist: BaseDistribution) -> str:\n    \"\"\"Serialize a distribution to JSON format.\n\n    Args:\n        dist: A distribution to be serialized.\n\n    Returns:\n        A JSON string of a given distribution.\n\n    \"\"\"\n\n    return json.dumps({\"name\": dist.__class__.__name__, \"attributes\": dist._asdict()})\n\n\ndef check_distribution_compatibility(\n    dist_old: BaseDistribution, dist_new: BaseDistribution\n) -> None:\n    \"\"\"A function to check compatibility of two distributions.\n\n    It checks whether ``dist_old`` and ``dist_new`` are the same kind of distributions.\n    If ``dist_old`` is :class:`~optuna.distributions.CategoricalDistribution`,\n    it further checks ``choices`` are the same between ``dist_old`` and ``dist_new``.\n    Note that this method is not supposed to be called by library users.\n\n    Args:\n        dist_old:\n            A distribution previously recorded in storage.\n        dist_new:\n            A distribution newly added to storage.\n\n    \"\"\"\n\n    if dist_old.__class__ != dist_new.__class__:\n        raise ValueError(\"Cannot set different distribution kind to the same parameter name.\")\n\n    if isinstance(dist_old, (FloatDistribution, IntDistribution)):\n        # For mypy.\n        assert isinstance(dist_new, (FloatDistribution, IntDistribution))\n\n        if dist_old.log != dist_new.log:\n            raise ValueError(\"Cannot set different log configuration to the same parameter name.\")\n\n    if not isinstance(dist_old, CategoricalDistribution):\n        return\n    if not isinstance(dist_new, CategoricalDistribution):\n        return\n    if dist_old != dist_new:\n        raise ValueError(\n            CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n        )\n\n\ndef _adjust_discrete_uniform_high(low: float, high: float, step: float) -> float:\n    d_high = decimal.Decimal(str(high))\n    d_low = decimal.Decimal(str(low))\n    d_step = decimal.Decimal(str(step))\n\n    d_r = d_high - d_low\n\n    if d_r % d_step != decimal.Decimal(\"0\"):\n        old_high = high\n        high = float((d_r // d_step) * d_step + d_low)\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n\n    return high\n\n\ndef _adjust_int_uniform_high(low: int, high: int, step: int) -> int:\n    r = high - low\n    if r % step != 0:\n        old_high = high\n        high = r // step * step + low\n        warnings.warn(\n            \"The distribution is specified by [{low}, {old_high}] and step={step}, but the range \"\n            \"is not divisible by `step`. It will be replaced by [{low}, {high}].\".format(\n                low=low, old_high=old_high, high=high, step=step\n            )\n        )\n    return high\n\n\ndef _get_single_value(distribution: BaseDistribution) -> int | float | CategoricalChoiceType:\n    assert distribution.single()\n\n    if isinstance(\n        distribution,\n        (\n            FloatDistribution,\n            IntDistribution,\n        ),\n    ):\n        return distribution.low\n    elif isinstance(distribution, CategoricalDistribution):\n        return distribution.choices[0]\n    assert False\n\n\n# TODO(himkt): Remove this method with the deletion of deprecated distributions.\n# https://github.com/optuna/optuna/issues/2941\ndef _convert_old_distribution_to_new_distribution(\n    distribution: BaseDistribution,\n    suppress_warning: bool = False,\n) -> BaseDistribution:\n    new_distribution: BaseDistribution\n\n    # Float distributions.\n    if isinstance(distribution, UniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=None,\n        )\n    elif isinstance(distribution, LogUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=None,\n        )\n    elif isinstance(distribution, DiscreteUniformDistribution):\n        new_distribution = FloatDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.q,\n        )\n\n    # Integer distributions.\n    elif isinstance(distribution, IntUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=False,\n            step=distribution.step,\n        )\n    elif isinstance(distribution, IntLogUniformDistribution):\n        new_distribution = IntDistribution(\n            low=distribution.low,\n            high=distribution.high,\n            log=True,\n            step=distribution.step,\n        )\n\n    # Categorical distribution.\n    else:\n        new_distribution = distribution\n\n    if new_distribution != distribution and not suppress_warning:\n        message = (\n            f\"{distribution} is deprecated and internally converted to\"\n            f\" {new_distribution}. See https://github.com/optuna/optuna/issues/2941.\"\n        )\n        warnings.warn(message, FutureWarning)\n\n    return new_distribution\n\n\ndef _is_distribution_log(distribution: BaseDistribution) -> bool:\n    if isinstance(distribution, (FloatDistribution, IntDistribution)):\n        return distribution.log\n\n    return False\n\n```\n\n# Answer (only for the method '_contains' of the class \"FloatDistribution\"):": "import unittest\nfrom distributions import FloatDistribution\n\nclass TestFloatDistributionContains(unittest.TestCase):\n    def test_continuous_contains(self):\n        dist = FloatDistribution(low=1.0, high=5.0, log=False, step=None)\n        self.assertTrue(dist._contains(1.0))\n        self.assertTrue(dist._contains(3.0))\n        self.assertTrue(dist._contains(5.0))\n        self.assertFalse(dist._contains(0.9))\n        self.assertFalse(dist._contains(5.1))\n\n    def test_discrete_contains_valid(self):\n        dist = FloatDistribution(low=1.0, high=5.0, step=1.0)\n        self.assertTrue(dist._contains(1.0))\n        self.assertTrue(dist._contains(2.0))\n        self.assertTrue(dist._contains(5.0))\n        self.assertTrue(dist._contains(3.0))\n\n    def test_discrete_contains_invalid(self):\n        dist = FloatDistribution(low=1.0, high=5.0, step=1.0)\n        self.assertFalse(dist._contains(1.5))\n        self.assertFalse(dist._contains(5.1))\n        self.assertFalse(dist._contains(0.5))\n\n    def test_discrete_with_tolerance(self):\n        dist = FloatDistribution(low=0.0, high=0.2, step=0.1)\n        self.assertTrue(dist._contains(0.1))\n        value = 0.1 + 0.5e-8\n        self.assertTrue(dist._contains(value))\n        value = 0.1 + 1.0e-8 + 1e-10\n        self.assertFalse(dist._contains(value))\n\n    def test_high_adjusted_for_step(self):\n        dist = FloatDistribution(low=1.0, high=5.5, step=1.0)\n        self.assertEqual(dist.high, 5.0)\n        self.assertTrue(dist._contains(5.0))\n        self.assertFalse(dist._contains(5.5))\n"}